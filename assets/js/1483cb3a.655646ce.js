(self.webpackChunksynapseml=self.webpackChunksynapseml||[]).push([[3022,2875,478,1117,2610,8582,1842,1043],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return c},kt:function(){return d}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),m=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},c=function(e){var t=m(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),p=m(n),d=o,f=p["".concat(l,".").concat(d)]||p[d]||u[d]||r;return n?a.createElement(f,s(s({ref:t},c),{},{components:n})):a.createElement(f,s({ref:t},c))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,s=new Array(r);s[0]=p;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,s[1]=i;for(var m=2;m<r;m++)s[m]=n[m];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},1332:function(e,t,n){"use strict";var a=n(7294);t.Z=function(e){var t=e.children,n=e.hidden,o=e.className;return a.createElement("div",{role:"tabpanel",hidden:n,className:o},t)}},5386:function(e,t,n){"use strict";n.d(t,{Z:function(){return u}});var a=n(7294),o=n(8578);var r=function(){var e=(0,a.useContext)(o.Z);if(null==e)throw new Error('"useUserPreferencesContext" is used outside of "Layout" component.');return e},s=n(6010),i="tabItem_2kG2",l="tabItemActive_3NDg";var m=37,c=39;var u=function(e){var t=e.lazy,n=e.block,o=e.defaultValue,u=e.values,p=e.groupId,d=e.className,f=r(),y=f.tabGroupChoices,v=f.setTabGroupChoices,g=(0,a.useState)(o),k=g[0],b=g[1],h=a.Children.toArray(e.children),T=[];if(null!=p){var M=y[p];null!=M&&M!==k&&u.some((function(e){return e.value===M}))&&b(M)}var C=function(e){var t=e.currentTarget,n=T.indexOf(t),a=u[n].value;b(a),null!=p&&(v(p,a),setTimeout((function(){var e,n,a,o,r,s,i,m;(e=t.getBoundingClientRect(),n=e.top,a=e.left,o=e.bottom,r=e.right,s=window,i=s.innerHeight,m=s.innerWidth,n>=0&&r<=m&&o<=i&&a>=0)||(t.scrollIntoView({block:"center",behavior:"smooth"}),t.classList.add(l),setTimeout((function(){return t.classList.remove(l)}),2e3))}),150))},I=function(e){var t,n;switch(e.keyCode){case c:var a=T.indexOf(e.target)+1;n=T[a]||T[0];break;case m:var o=T.indexOf(e.target)-1;n=T[o]||T[T.length-1]}null==(t=n)||t.focus()};return a.createElement("div",{className:"tabs-container"},a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.Z)("tabs",{"tabs--block":n},d)},u.map((function(e){var t=e.value,n=e.label;return a.createElement("li",{role:"tab",tabIndex:k===t?0:-1,"aria-selected":k===t,className:(0,s.Z)("tabs__item",i,{"tabs__item--active":k===t}),key:t,ref:function(e){return T.push(e)},onKeyDown:I,onFocus:C,onClick:C},n)}))),t?(0,a.cloneElement)(h.filter((function(e){return e.props.value===k}))[0],{className:"margin-vert--md"}):a.createElement("div",{className:"margin-vert--md"},h.map((function(e,t){return(0,a.cloneElement)(e,{key:t,hidden:e.props.value!==k})}))))}},8578:function(e,t,n){"use strict";var a=(0,n(7294).createContext)(void 0);t.Z=a},1989:function(e,t,n){"use strict";var a=n(7294),o=n(2263);t.Z=function(e){var t=e.className,n=e.py,r=e.scala,s=e.sourceLink,i=(0,o.Z)().siteConfig.customFields.version,l="https://mmlspark.blob.core.windows.net/docs/"+i+"/pyspark/"+n,m="https://mmlspark.blob.core.windows.net/docs/"+i+"/scala/"+r;return a.createElement("table",null,a.createElement("tbody",null,a.createElement("tr",null,a.createElement("td",null,a.createElement("strong",null,"Python API: "),a.createElement("a",{href:l},t)),a.createElement("td",null,a.createElement("strong",null,"Scala API: "),a.createElement("a",{href:m},t)),a.createElement("td",null,a.createElement("strong",null,"Source: "),a.createElement("a",{href:s},t)))))}},5207:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return p},toc:function(){return d},default:function(){return y}});var a=n(2122),o=n(9756),r=(n(7294),n(3905)),s=n(5386),i=n(1332),l=n(1989),m=["components"],c={},u=void 0,p={unversionedId:"documentation/estimators/core/_AutoML",id:"version-0.9.1/documentation/estimators/core/_AutoML",isDocsHomePage:!1,title:"_AutoML",description:"\x3c!--",source:"@site/versioned_docs/version-0.9.1/documentation/estimators/core/_AutoML.md",sourceDirName:"documentation/estimators/core",slug:"/documentation/estimators/core/_AutoML",permalink:"/docs/documentation/estimators/core/_AutoML",version:"0.9.1",frontMatter:{}},d=[{value:"AutoML",id:"automl",children:[{value:"FindBestModel",id:"findbestmodel",children:[]},{value:"TuneHyperparameters",id:"tunehyperparameters",children:[]}]}],f={toc:d};function y(e){var t=e.components,n=(0,o.Z)(e,m);return(0,r.kt)("wrapper",(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"automl"},"AutoML"),(0,r.kt)("h3",{id:"findbestmodel"},"FindBestModel"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.automl import *\nfrom synapse.ml.train import *\nfrom pyspark.ml.classification import RandomForestClassifier\n\ndf = (spark.createDataFrame([\n    (0, 2, 0.50, 0.60, 0),\n    (1, 3, 0.40, 0.50, 1),\n    (0, 4, 0.78, 0.99, 2),\n    (1, 5, 0.12, 0.34, 3),\n    (0, 1, 0.50, 0.60, 0),\n    (1, 3, 0.40, 0.50, 1),\n    (0, 3, 0.78, 0.99, 2),\n    (1, 4, 0.12, 0.34, 3),\n    (0, 0, 0.50, 0.60, 0),\n    (1, 2, 0.40, 0.50, 1),\n    (0, 3, 0.78, 0.99, 2),\n    (1, 4, 0.12, 0.34, 3)\n], ["Label", "col1", "col2", "col3", "col4"]))\n\n# mocking models\nrandomForestClassifier = (TrainClassifier()\n      .setModel(RandomForestClassifier()\n        .setMaxBins(32)\n        .setMaxDepth(5)\n        .setMinInfoGain(0.0)\n        .setMinInstancesPerNode(1)\n        .setNumTrees(20)\n        .setSubsamplingRate(1.0)\n        .setSeed(0))\n      .setFeaturesCol("mlfeatures")\n      .setLabelCol("Label"))\nmodel = randomForestClassifier.fit(df)\n\nfindBestModel = (FindBestModel()\n  .setModels([model, model])\n  .setEvaluationMetric("accuracy"))\nbestModel = findBestModel.fit(df)\ndisplay(bestModel.transform(df))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.automl._\nimport com.microsoft.azure.synapse.ml.train._\nimport spark.implicits._\nimport org.apache.spark.ml.Transformer\n\nval df = (Seq(\n      (0, 2, 0.50, 0.60, 0),\n      (1, 3, 0.40, 0.50, 1),\n      (0, 4, 0.78, 0.99, 2),\n      (1, 5, 0.12, 0.34, 3),\n      (0, 1, 0.50, 0.60, 0),\n      (1, 3, 0.40, 0.50, 1),\n      (0, 3, 0.78, 0.99, 2),\n      (1, 4, 0.12, 0.34, 3),\n      (0, 0, 0.50, 0.60, 0),\n      (1, 2, 0.40, 0.50, 1),\n      (0, 3, 0.78, 0.99, 2),\n      (1, 4, 0.12, 0.34, 3)\n  ).toDF("Label", "col1", "col2", "col3", "col4"))\n\n// mocking models\nval randomForestClassifier = (new TrainClassifier()\n      .setModel(\n        new RandomForestClassifier()\n        .setMaxBins(32)\n        .setMaxDepth(5)\n        .setMinInfoGain(0.0)\n        .setMinInstancesPerNode(1)\n        .setNumTrees(20)\n        .setSubsamplingRate(1.0)\n        .setSeed(0L))\n      .setFeaturesCol("mlfeatures")\n      .setLabelCol("Label"))\nval model = randomForestClassifier.fit(df)\n\nval findBestModel = (new FindBestModel()\n  .setModels(Array(model.asInstanceOf[Transformer], model.asInstanceOf[Transformer]))\n  .setEvaluationMetric("accuracy"))\nval bestModel = findBestModel.fit(df)\ndisplay(bestModel.transform(df))\n')))),(0,r.kt)(l.Z,{className:"FindBestModel",py:"synapse.ml.automl.html#module-synapse.ml.automl.FindBestModel",scala:"com/microsoft/azure/synapse/ml/automl/FindBestModel.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/automl/FindBestModel.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"tunehyperparameters"},"TuneHyperparameters"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.automl import *\nfrom synapse.ml.train import *\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n\n\ndf = (spark.createDataFrame([\n    (0, 1, 1, 1, 1, 1, 1.0, 3, 1, 1),\n    (0, 1, 1, 1, 1, 2, 1.0, 1, 1, 1),\n    (0, 1, 1, 1, 1, 2, 1.0, 2, 1, 1),\n    (0, 1, 2, 3, 1, 2, 1.0, 3, 1, 1),\n    (0, 3, 1, 1, 1, 2, 1.0, 3, 1, 1)\n], ["Label", "Clump_Thickness", "Uniformity_of_Cell_Size",\n    "Uniformity_of_Cell_Shape", "Marginal_Adhesion", "Single_Epithelial_Cell_Size",\n    "Bare_Nuclei", "Bland_Chromatin", "Normal_Nucleoli", "Mitoses"]))\n\nlogReg = LogisticRegression()\nrandForest = RandomForestClassifier()\ngbt = GBTClassifier()\nsmlmodels = [logReg, randForest, gbt]\nmmlmodels = [TrainClassifier(model=model, labelCol="Label") for model in smlmodels]\n\nparamBuilder = (HyperparamBuilder()\n    .addHyperparam(logReg, logReg.regParam, RangeHyperParam(0.1, 0.3))\n    .addHyperparam(randForest, randForest.numTrees, DiscreteHyperParam([5,10]))\n    .addHyperparam(randForest, randForest.maxDepth, DiscreteHyperParam([3,5]))\n    .addHyperparam(gbt, gbt.maxBins, RangeHyperParam(8,16))\n    .addHyperparam(gbt, gbt.maxDepth, DiscreteHyperParam([3,5])))\nsearchSpace = paramBuilder.build()\n# The search space is a list of params to tuples of estimator and hyperparam\nrandomSpace = RandomSpace(searchSpace)\n\nbestModel = TuneHyperparameters(\n              evaluationMetric="accuracy", models=mmlmodels, numFolds=2,\n              numRuns=len(mmlmodels) * 2, parallelism=2,\n              paramSpace=randomSpace.space(), seed=0).fit(df)\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.automl._\nimport com.microsoft.azure.synapse.ml.train._\nimport spark.implicits._\n\nval logReg = new LogisticRegression()\nval randForest = new RandomForestClassifier()\nval gbt = new GBTClassifier()\nval smlmodels = Seq(logReg, randForest, gbt)\nval mmlmodels = smlmodels.map(model => new TrainClassifier().setModel(model).setLabelCol("Label"))\n\nval paramBuilder = new HyperparamBuilder()\n  .addHyperparam(logReg.regParam, new DoubleRangeHyperParam(0.1, 0.3))\n  .addHyperparam(randForest.numTrees, new DiscreteHyperParam(List(5,10)))\n  .addHyperparam(randForest.maxDepth, new DiscreteHyperParam(List(3,5)))\n  .addHyperparam(gbt.maxBins, new IntRangeHyperParam(8,16))\n.addHyperparam(gbt.maxDepth, new DiscreteHyperParam(List(3,5)))\nval searchSpace = paramBuilder.build()\nval randomSpace = new RandomSpace(searchSpace)\n\nval dataset: DataFrame = Seq(\n  (0, 1, 1, 1, 1, 1, 1.0, 3, 1, 1),\n  (0, 1, 1, 1, 1, 2, 1.0, 1, 1, 1),\n  (0, 1, 1, 1, 1, 2, 1.0, 2, 1, 1),\n  (0, 1, 2, 3, 1, 2, 1.0, 3, 1, 1),\n  (0, 3, 1, 1, 1, 2, 1.0, 3, 1, 1))\n  .toDF("Label", "Clump_Thickness", "Uniformity_of_Cell_Size",\n    "Uniformity_of_Cell_Shape", "Marginal_Adhesion", "Single_Epithelial_Cell_Size",\n    "Bare_Nuclei", "Bland_Chromatin", "Normal_Nucleoli", "Mitoses")\n\nval tuneHyperparameters = new TuneHyperparameters().setEvaluationMetric("accuracy")\n  .setModels(mmlmodels.toArray).setNumFolds(2).setNumRuns(mmlmodels.length * 2)\n  .setParallelism(1).setParamSpace(randomSpace).setSeed(0)\ndisplay(tuneHyperparameters.fit(dataset))\n')))),(0,r.kt)(l.Z,{className:"TuneHyperparameters",py:"synapse.ml.automl.html#module-synapse.ml.automl.TuneHyperparameters",scala:"com/microsoft/azure/synapse/ml/automl/TuneHyperparameters.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/automl/TuneHyperparameters.scala",mdxType:"DocTable"}))}y.isMDXComponent=!0},8830:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return p},toc:function(){return d},default:function(){return y}});var a=n(2122),o=n(9756),r=(n(7294),n(3905)),s=n(5386),i=n(1332),l=n(1989),m=["components"],c={},u=void 0,p={unversionedId:"documentation/estimators/core/_Featurize",id:"version-0.9.1/documentation/estimators/core/_Featurize",isDocsHomePage:!1,title:"_Featurize",description:"\x3c!--",source:"@site/versioned_docs/version-0.9.1/documentation/estimators/core/_Featurize.md",sourceDirName:"documentation/estimators/core",slug:"/documentation/estimators/core/_Featurize",permalink:"/docs/documentation/estimators/core/_Featurize",version:"0.9.1",frontMatter:{}},d=[{value:"Featurize",id:"featurize",children:[{value:"CleanMissingData",id:"cleanmissingdata",children:[]},{value:"CountSelector",id:"countselector",children:[]},{value:"Featurize",id:"featurize-1",children:[]},{value:"ValueIndexer",id:"valueindexer",children:[]}]},{value:"Featurize Text",id:"featurize-text",children:[{value:"TextFeaturizer",id:"textfeaturizer",children:[]}]}],f={toc:d};function y(e){var t=e.components,n=(0,o.Z)(e,m);return(0,r.kt)("wrapper",(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"featurize"},"Featurize"),(0,r.kt)("h3",{id:"cleanmissingdata"},"CleanMissingData"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.featurize import *\n\ndataset = spark.createDataFrame([\n    (0,    2,    0.50, 0.60, 0),\n    (1,    3,    0.40, None, None),\n    (0,    4,    0.78, 0.99, 2),\n    (1,    5,    0.12, 0.34, 3),\n    (0,    1,    0.50, 0.60, 0),\n    (None, None, None, None, None),\n    (0,    3,    0.78, 0.99, 2),\n    (1,    4,    0.12, 0.34, 3),\n    (0,    None, 0.50, 0.60, 0),\n    (1,    2,    0.40, 0.50, None),\n    (0,    3,    None, 0.99, 2),\n    (1,    4,    0.12, 0.34, 3)\n], ["col1", "col2", "col3", "col4", "col5"])\n\ncmd = (CleanMissingData()\n      .setInputCols(dataset.columns)\n      .setOutputCols(dataset.columns)\n      .setCleaningMode("Mean"))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.featurize._\nimport java.lang.{Boolean => JBoolean, Double => JDouble, Integer => JInt}\nimport spark.implicits._\n\ndef createMockDataset: DataFrame = {\n    Seq[(JInt, JInt, JDouble, JDouble, JInt)](\n      (0,    2,    0.50, 0.60, 0),\n      (1,    3,    0.40, null, null),\n      (0,    4,    0.78, 0.99, 2),\n      (1,    5,    0.12, 0.34, 3),\n      (0,    1,    0.50, 0.60, 0),\n      (null, null, null, null, null),\n      (0,    3,    0.78, 0.99, 2),\n      (1,    4,    0.12, 0.34, 3),\n      (0,    null, 0.50, 0.60, 0),\n      (1,    2,    0.40, 0.50, null),\n      (0,    3,    null, 0.99, 2),\n      (1,    4,    0.12, 0.34, 3))\n      .toDF("col1", "col2", "col3", "col4", "col5")\n  }\n\nval dataset = createMockDataset\nval cmd = (new CleanMissingData()\n      .setInputCols(dataset.columns)\n      .setOutputCols(dataset.columns)\n      .setCleaningMode("Mean"))\n')))),(0,r.kt)(l.Z,{className:"CleanMissingData",py:"synapse.ml.featurize.html#module-synapse.ml.featurize.CleanMissingData",scala:"com/microsoft/azure/synapse/ml/featurize/CleanMissingData.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/featurize/CleanMissingData.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"countselector"},"CountSelector"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.featurize import *\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n    (Vectors.sparse(3, [(0, 1.0), (2, 2.0)]), Vectors.dense(1.0, 0.1, 0)),\n    (Vectors.sparse(3, [(0, 1.0), (2, 2.0)]), Vectors.dense(1.0, 0.1, 0))\n], ["col1", "col2"])\n\ncs = CountSelector().setInputCol("col1").setOutputCol("col3")\n\ndisplay(cs.fit(df).transform(df))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.featurize._\nimport org.apache.spark.ml.linalg.Vectors\nimport spark.implicits._\n\nval df = Seq(\n    (Vectors.sparse(3, Seq((0, 1.0), (2, 2.0))), Vectors.dense(1.0, 0.1, 0)),\n    (Vectors.sparse(3, Seq((0, 1.0), (2, 2.0))), Vectors.dense(1.0, 0.1, 0))\n  ).toDF("col1", "col2")\n\nval cs = (new CountSelector()\n            .setInputCol("col1")\n            .setOutputCol("col3"))\n\ndisplay(cs.fit(df).transform(df))\n')))),(0,r.kt)(l.Z,{className:"CountSelector",py:"synapse.ml.featurize.html#module-synapse.ml.featurize.CountSelector",scala:"com/microsoft/azure/synapse/ml/featurize/CountSelector.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/featurize/CountSelector.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"featurize-1"},"Featurize"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.featurize import *\n\ndataset = spark.createDataFrame([\n    (0, 2, 0.50, 0.60, "pokemon are everywhere"),\n    (1, 3, 0.40, 0.50, "they are in the woods"),\n    (0, 4, 0.78, 0.99, "they are in the water"),\n    (1, 5, 0.12, 0.34, "they are in the fields"),\n    (0, 3, 0.78, 0.99, "pokemon - gotta catch em all")\n], ["Label", "col1", "col2", "col3"])\n\nfeatureColumns = filter(lambda x: x != "Label", dataset.columns)\n\nfeat = (Featurize()\n      .setNumFeatures(10)\n      .setOutputCol("testColumn")\n      .setInputCols(list(featureColumns))\n      .setOneHotEncodeCategoricals(False))\n\ndisplay(feat.fit(dataset).transform(dataset))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.featurize._\nimport spark.implicits._\n\nval dataset = Seq(\n      (0, 2, 0.50, 0.60, "pokemon are everywhere"),\n      (1, 3, 0.40, 0.50, "they are in the woods"),\n      (0, 4, 0.78, 0.99, "they are in the water"),\n      (1, 5, 0.12, 0.34, "they are in the fields"),\n      (0, 3, 0.78, 0.99, "pokemon - gotta catch em all")).toDF("Label", "col1", "col2", "col3")\n\nval featureColumns = dataset.columns.filter(_ != "Label")\n\nval feat = (new Featurize()\n      .setNumFeatures(10)\n      .setOutputCol("testColumn")\n      .setInputCols(featureColumns)\n      .setOneHotEncodeCategoricals(false))\n\ndisplay(feat.fit(dataset).transform(dataset))\n')))),(0,r.kt)(l.Z,{className:"Featurize",py:"synapse.ml.featurize.html#module-synapse.ml.featurize.Featurize",scala:"com/microsoft/azure/synapse/ml/featurize/Featurize.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/featurize/Featurize.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"valueindexer"},"ValueIndexer"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.featurize import *\n\ndf = spark.createDataFrame([\n    (-3, 24, 0.32534, True, "piano"),\n    (1, 5, 5.67, False, "piano"),\n    (-3, 5, 0.32534, False, "guitar")\n], ["int", "long", "double", "bool", "string"])\n\nvi = ValueIndexer().setInputCol("string").setOutputCol("string_cat")\n\ndisplay(vi.fit(df).transform(df))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.featurize._\nimport spark.implicits._\n\nval df = Seq[(Int, Long, Double, Boolean, String)](\n    (-3, 24L, 0.32534, true, "piano"),\n    (1, 5L, 5.67, false, "piano"),\n    (-3, 5L, 0.32534, false, "guitar")).toDF("int", "long", "double", "bool", "string")\n\nval vi = new ValueIndexer().setInputCol("string").setOutputCol("string_cat")\n\ndisplay(vi.fit(df).transform(df))\n')))),(0,r.kt)(l.Z,{className:"ValueIndexer",py:"synapse.ml.featurize.html#module-synapse.ml.featurize.ValueIndexer",scala:"com/microsoft/azure/synapse/ml/featurize/ValueIndexer.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/featurize/ValueIndexer.scala",mdxType:"DocTable"}),(0,r.kt)("h2",{id:"featurize-text"},"Featurize Text"),(0,r.kt)("h3",{id:"textfeaturizer"},"TextFeaturizer"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.featurize.text import *\n\ndfRaw = spark.createDataFrame([\n    (0, "Hi I"),\n    (1, "I wish for snow today"),\n    (2, "we Cant go to the park, because of the snow!"),\n    (3, "")\n], ["label", "sentence"])\n\ntfRaw = (TextFeaturizer()\n      .setInputCol("sentence")\n      .setOutputCol("features")\n      .setNumFeatures(20))\n\ndisplay(tfRaw.fit(dfRaw).transform(dfRaw))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.featurize.text._\nimport spark.implicits._\n\nval dfRaw = Seq((0, "Hi I"),\n            (1, "I wish for snow today"),\n            (2, "we Cant go to the park, because of the snow!"),\n            (3, "")).toDF("label", "sentence")\n\nval tfRaw = (new TextFeaturizer()\n      .setInputCol("sentence")\n      .setOutputCol("features")\n      .setNumFeatures(20))\n\ndisplay(tfRaw.fit(dfRaw).transform(dfRaw))\n')))),(0,r.kt)(l.Z,{className:"TextFeaturizer",py:"synapse.ml.featurize.text.html#module-synapse.ml.featurize.text.TextFeaturizer",scala:"com/microsoft/azure/synapse/ml/featurize/text/TextFeaturizer.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/featurize/text/TextFeaturizer.scala",mdxType:"DocTable"}))}y.isMDXComponent=!0},2569:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return p},toc:function(){return d},default:function(){return y}});var a=n(2122),o=n(9756),r=(n(7294),n(3905)),s=n(5386),i=n(1332),l=n(1989),m=["components"],c={},u=void 0,p={unversionedId:"documentation/estimators/core/_IsolationForest",id:"version-0.9.1/documentation/estimators/core/_IsolationForest",isDocsHomePage:!1,title:"_IsolationForest",description:"\x3c!--",source:"@site/versioned_docs/version-0.9.1/documentation/estimators/core/_IsolationForest.md",sourceDirName:"documentation/estimators/core",slug:"/documentation/estimators/core/_IsolationForest",permalink:"/docs/documentation/estimators/core/_IsolationForest",version:"0.9.1",frontMatter:{}},d=[{value:"Isolation Forest",id:"isolation-forest",children:[{value:"IsolationForest",id:"isolationforest",children:[]}]}],f={toc:d};function y(e){var t=e.components,n=(0,o.Z)(e,m);return(0,r.kt)("wrapper",(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"isolation-forest"},"Isolation Forest"),(0,r.kt)("h3",{id:"isolationforest"},"IsolationForest"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.isolationforest import *\n\nisolationForest = (IsolationForest()\n      .setNumEstimators(100)\n      .setBootstrap(False)\n      .setMaxSamples(256)\n      .setMaxFeatures(1.0)\n      .setFeaturesCol("features")\n      .setPredictionCol("predictedLabel")\n      .setScoreCol("outlierScore")\n      .setContamination(0.02)\n      .setContaminationError(0.02 * 0.01)\n      .setRandomSeed(1))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.isolationforest._\nimport spark.implicits._\n\nval isolationForest = (new IsolationForest()\n      .setNumEstimators(100)\n      .setBootstrap(false)\n      .setMaxSamples(256)\n      .setMaxFeatures(1.0)\n      .setFeaturesCol("features")\n      .setPredictionCol("predictedLabel")\n      .setScoreCol("outlierScore")\n      .setContamination(0.02)\n      .setContaminationError(0.02 * 0.01)\n      .setRandomSeed(1))\n')))),(0,r.kt)(l.Z,{className:"CleanMissingData",py:"mmlspark.isolationforest.html#module-mmlspark.isolationforest.IsolationForest",scala:"com/microsoft/azure/synapse/ml/isolationforest/IsolationForest.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/isolationforest/IsolationForest.scala",mdxType:"DocTable"}))}y.isMDXComponent=!0},6741:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return p},toc:function(){return d},default:function(){return y}});var a=n(2122),o=n(9756),r=(n(7294),n(3905)),s=n(5386),i=n(1332),l=n(1989),m=["components"],c={},u=void 0,p={unversionedId:"documentation/estimators/core/_NN",id:"version-0.9.1/documentation/estimators/core/_NN",isDocsHomePage:!1,title:"_NN",description:"\x3c!--",source:"@site/versioned_docs/version-0.9.1/documentation/estimators/core/_NN.md",sourceDirName:"documentation/estimators/core",slug:"/documentation/estimators/core/_NN",permalink:"/docs/documentation/estimators/core/_NN",version:"0.9.1",frontMatter:{}},d=[{value:"NN",id:"nn",children:[{value:"ConditionalKNN",id:"conditionalknn",children:[]},{value:"KNN",id:"knn",children:[]}]}],f={toc:d};function y(e){var t=e.components,n=(0,o.Z)(e,m);return(0,r.kt)("wrapper",(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"nn"},"NN"),(0,r.kt)("h3",{id:"conditionalknn"},"ConditionalKNN"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.nn import *\n\ncknn = (ConditionalKNN()\n      .setOutputCol("matches")\n      .setFeaturesCol("features"))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.nn._\nimport spark.implicits._\n\nval cknn = (new ConditionalKNN()\n            .setOutputCol("matches")\n            .setFeaturesCol("features"))\n')))),(0,r.kt)(l.Z,{className:"ConditionalKNN",py:"mmlspark.nn.html#module-mmlspark.nn.ConditionalKNN",scala:"com/microsoft/azure/synapse/ml/nn/ConditionalKNN.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/nn/ConditionalKNN.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"knn"},"KNN"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.nn import *\n\nknn = (KNN()\n      .setOutputCol("matches"))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.nn._\nimport spark.implicits._\n\nval knn = (new KNN()\n      .setOutputCol("matches"))\n')))),(0,r.kt)(l.Z,{className:"KNN",py:"mmlspark.nn.html#module-mmlspark.nn.KNN",scala:"com/microsoft/azure/synapse/ml/nn/KNN.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/nn/KNN.scala",mdxType:"DocTable"}))}y.isMDXComponent=!0},1522:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return p},toc:function(){return d},default:function(){return y}});var a=n(2122),o=n(9756),r=(n(7294),n(3905)),s=n(5386),i=n(1332),l=n(1989),m=["components"],c={},u=void 0,p={unversionedId:"documentation/estimators/core/_Recommendation",id:"version-0.9.1/documentation/estimators/core/_Recommendation",isDocsHomePage:!1,title:"_Recommendation",description:"\x3c!--",source:"@site/versioned_docs/version-0.9.1/documentation/estimators/core/_Recommendation.md",sourceDirName:"documentation/estimators/core",slug:"/documentation/estimators/core/_Recommendation",permalink:"/docs/documentation/estimators/core/_Recommendation",version:"0.9.1",frontMatter:{}},d=[{value:"Recommendation",id:"recommendation",children:[{value:"RecommendationIndexer, RankingEvaluator, RankingAdapter and RankingTrainValidationSplit",id:"recommendationindexer-rankingevaluator-rankingadapter-and-rankingtrainvalidationsplit",children:[]},{value:"SAR",id:"sar",children:[]}]}],f={toc:d};function y(e){var t=e.components,n=(0,o.Z)(e,m);return(0,r.kt)("wrapper",(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"recommendation"},"Recommendation"),(0,r.kt)("h3",{id:"recommendationindexer-rankingevaluator-rankingadapter-and-rankingtrainvalidationsplit"},"RecommendationIndexer, RankingEvaluator, RankingAdapter and RankingTrainValidationSplit"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.recommendation import *\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import *\n\nratings = (spark.createDataFrame([\n      ("11", "Movie 01", 2),\n      ("11", "Movie 03", 1),\n      ("11", "Movie 04", 5),\n      ("11", "Movie 05", 3),\n      ("11", "Movie 06", 4),\n      ("11", "Movie 07", 1),\n      ("11", "Movie 08", 5),\n      ("11", "Movie 09", 3),\n      ("22", "Movie 01", 4),\n      ("22", "Movie 02", 5),\n      ("22", "Movie 03", 1),\n      ("22", "Movie 05", 3),\n      ("22", "Movie 06", 3),\n      ("22", "Movie 07", 5),\n      ("22", "Movie 08", 1),\n      ("22", "Movie 10", 3),\n      ("33", "Movie 01", 4),\n      ("33", "Movie 03", 1),\n      ("33", "Movie 04", 5),\n      ("33", "Movie 05", 3),\n      ("33", "Movie 06", 4),\n      ("33", "Movie 08", 1),\n      ("33", "Movie 09", 5),\n      ("33", "Movie 10", 3),\n      ("44", "Movie 01", 4),\n      ("44", "Movie 02", 5),\n      ("44", "Movie 03", 1),\n      ("44", "Movie 05", 3),\n      ("44", "Movie 06", 4),\n      ("44", "Movie 07", 5),\n      ("44", "Movie 08", 1),\n      ("44", "Movie 10", 3)\n      ], ["customerIDOrg", "itemIDOrg", "rating"])\n    .dropDuplicates()\n    .cache())\n\nrecommendationIndexer = (RecommendationIndexer()\n    .setUserInputCol("customerIDOrg")\n    .setUserOutputCol("customerID")\n    .setItemInputCol("itemIDOrg")\n    .setItemOutputCol("itemID")\n    .setRatingCol("rating"))\n\ntransformedDf = (recommendationIndexer.fit(ratings)\n    .transform(ratings).cache())\n\nals = (ALS()\n    .setNumUserBlocks(1)\n    .setNumItemBlocks(1)\n    .setUserCol("customerID")\n    .setItemCol("itemID")\n    .setRatingCol("rating")\n    .setSeed(0))\n\nevaluator = (RankingEvaluator()\n    .setK(3)\n    .setNItems(10))\n\nadapter = (RankingAdapter()\n    .setK(evaluator.getK())\n    .setRecommender(als))\n\ndisplay(adapter.fit(transformedDf).transform(transformedDf))\n\nparamGrid = (ParamGridBuilder()\n    .addGrid(als.regParam, [1.0])\n    .build())\n\ntvRecommendationSplit = (RankingTrainValidationSplit()\n      .setEstimator(als)\n      .setEvaluator(evaluator)\n      .setEstimatorParamMaps(paramGrid)\n      .setTrainRatio(0.8)\n      .setUserCol(recommendationIndexer.getUserOutputCol())\n      .setItemCol(recommendationIndexer.getItemOutputCol())\n      .setRatingCol("rating"))\n\ndisplay(tvRecommendationSplit.fit(transformedDf).transform(transformedDf))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.recommendation._\nimport org.apache.spark.ml.recommendation.ALS\nimport org.apache.spark.ml.tuning._\nimport spark.implicits._\n\nval ratings = (Seq(\n      ("11", "Movie 01", 2),\n      ("11", "Movie 03", 1),\n      ("11", "Movie 04", 5),\n      ("11", "Movie 05", 3),\n      ("11", "Movie 06", 4),\n      ("11", "Movie 07", 1),\n      ("11", "Movie 08", 5),\n      ("11", "Movie 09", 3),\n      ("22", "Movie 01", 4),\n      ("22", "Movie 02", 5),\n      ("22", "Movie 03", 1),\n      ("22", "Movie 05", 3),\n      ("22", "Movie 06", 3),\n      ("22", "Movie 07", 5),\n      ("22", "Movie 08", 1),\n      ("22", "Movie 10", 3),\n      ("33", "Movie 01", 4),\n      ("33", "Movie 03", 1),\n      ("33", "Movie 04", 5),\n      ("33", "Movie 05", 3),\n      ("33", "Movie 06", 4),\n      ("33", "Movie 08", 1),\n      ("33", "Movie 09", 5),\n      ("33", "Movie 10", 3),\n      ("44", "Movie 01", 4),\n      ("44", "Movie 02", 5),\n      ("44", "Movie 03", 1),\n      ("44", "Movie 05", 3),\n      ("44", "Movie 06", 4),\n      ("44", "Movie 07", 5),\n      ("44", "Movie 08", 1),\n      ("44", "Movie 10", 3))\n    .toDF("customerIDOrg", "itemIDOrg", "rating")\n    .dropDuplicates()\n    .cache())\n\nval recommendationIndexer = (new RecommendationIndexer()\n    .setUserInputCol("customerIDOrg")\n    .setUserOutputCol("customerID")\n    .setItemInputCol("itemIDOrg")\n    .setItemOutputCol("itemID")\n    .setRatingCol("rating"))\n\nval transformedDf = (recommendationIndexer.fit(ratings)\n    .transform(ratings).cache())\n\nval als = (new ALS()\n    .setNumUserBlocks(1)\n    .setNumItemBlocks(1)\n    .setUserCol("customerID")\n    .setItemCol("itemID")\n    .setRatingCol("rating")\n    .setSeed(0))\n\nval evaluator = (new RankingEvaluator()\n    .setK(3)\n    .setNItems(10))\n\nval adapter = (new RankingAdapter()\n    .setK(evaluator.getK)\n    .setRecommender(als))\n\ndisplay(adapter.fit(transformedDf).transform(transformedDf))\n\nval paramGrid = (new ParamGridBuilder()\n    .addGrid(als.regParam, Array(1.0))\n    .build())\n\nval tvRecommendationSplit = (new RankingTrainValidationSplit()\n      .setEstimator(als)\n      .setEvaluator(evaluator)\n      .setEstimatorParamMaps(paramGrid)\n      .setTrainRatio(0.8)\n      .setUserCol(recommendationIndexer.getUserOutputCol)\n      .setItemCol(recommendationIndexer.getItemOutputCol)\n      .setRatingCol("rating"))\n\ndisplay(tvRecommendationSplit.fit(transformedDf).transform(transformedDf))\n')))),(0,r.kt)(l.Z,{className:"RecommendationIndexer",py:"mmlspark.recommendation.html#module-mmlspark.recommendation.RecommendationIndexer",scala:"com/microsoft/azure/synapse/ml/recommendation/RecommendationIndexer.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/recommendation/RecommendationIndexer.scala",mdxType:"DocTable"}),(0,r.kt)(l.Z,{className:"RankingEvaluator",py:"mmlspark.recommendation.html#module-mmlspark.recommendation.RankingEvaluator",scala:"com/microsoft/azure/synapse/ml/recommendation/RankingEvaluator.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/recommendation/RankingEvaluator.scala",mdxType:"DocTable"}),(0,r.kt)(l.Z,{className:"RankingAdapter",py:"mmlspark.recommendation.html#module-mmlspark.recommendation.RankingAdapter",scala:"com/microsoft/azure/synapse/ml/recommendation/RankingAdapter.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/recommendation/RankingAdapter.scala",mdxType:"DocTable"}),(0,r.kt)(l.Z,{className:"RankingTrainValidationSplit",py:"mmlspark.recommendation.html#module-mmlspark.recommendation.RankingTrainValidationSplit",scala:"com/microsoft/azure/synapse/ml/recommendation/RankingTrainValidationSplit.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/recommendation/RankingTrainValidationSplit.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"sar"},"SAR"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.recommendation import *\n\nratings = (spark.createDataFrame([\n      ("11", "Movie 01", 2),\n      ("11", "Movie 03", 1),\n      ("11", "Movie 04", 5),\n      ("11", "Movie 05", 3),\n      ("11", "Movie 06", 4),\n      ("11", "Movie 07", 1),\n      ("11", "Movie 08", 5),\n      ("11", "Movie 09", 3),\n      ("22", "Movie 01", 4),\n      ("22", "Movie 02", 5),\n      ("22", "Movie 03", 1),\n      ("22", "Movie 05", 3),\n      ("22", "Movie 06", 3),\n      ("22", "Movie 07", 5),\n      ("22", "Movie 08", 1),\n      ("22", "Movie 10", 3),\n      ("33", "Movie 01", 4),\n      ("33", "Movie 03", 1),\n      ("33", "Movie 04", 5),\n      ("33", "Movie 05", 3),\n      ("33", "Movie 06", 4),\n      ("33", "Movie 08", 1),\n      ("33", "Movie 09", 5),\n      ("33", "Movie 10", 3),\n      ("44", "Movie 01", 4),\n      ("44", "Movie 02", 5),\n      ("44", "Movie 03", 1),\n      ("44", "Movie 05", 3),\n      ("44", "Movie 06", 4),\n      ("44", "Movie 07", 5),\n      ("44", "Movie 08", 1),\n      ("44", "Movie 10", 3)\n      ], ["customerIDOrg", "itemIDOrg", "rating"])\n    .dropDuplicates()\n    .cache())\n\nrecommendationIndexer = (RecommendationIndexer()\n    .setUserInputCol("customerIDOrg")\n    .setUserOutputCol("customerID")\n    .setItemInputCol("itemIDOrg")\n    .setItemOutputCol("itemID")\n    .setRatingCol("rating"))\n\nalgo = (SAR()\n      .setUserCol("customerID")\n      .setItemCol("itemID")\n      .setRatingCol("rating")\n      .setTimeCol("timestamp")\n      .setSupportThreshold(1)\n      .setSimilarityFunction("jacccard")\n      .setActivityTimeFormat("EEE MMM dd HH:mm:ss Z yyyy"))\n\nadapter = (RankingAdapter()\n      .setK(5)\n      .setRecommender(algo))\n\nres1 = recommendationIndexer.fit(ratings).transform(ratings).cache()\n\ndisplay(adapter.fit(res1).transform(res1))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.recommendation._\nimport spark.implicits._\n\nval ratings = (Seq(\n      ("11", "Movie 01", 2),\n      ("11", "Movie 03", 1),\n      ("11", "Movie 04", 5),\n      ("11", "Movie 05", 3),\n      ("11", "Movie 06", 4),\n      ("11", "Movie 07", 1),\n      ("11", "Movie 08", 5),\n      ("11", "Movie 09", 3),\n      ("22", "Movie 01", 4),\n      ("22", "Movie 02", 5),\n      ("22", "Movie 03", 1),\n      ("22", "Movie 05", 3),\n      ("22", "Movie 06", 3),\n      ("22", "Movie 07", 5),\n      ("22", "Movie 08", 1),\n      ("22", "Movie 10", 3),\n      ("33", "Movie 01", 4),\n      ("33", "Movie 03", 1),\n      ("33", "Movie 04", 5),\n      ("33", "Movie 05", 3),\n      ("33", "Movie 06", 4),\n      ("33", "Movie 08", 1),\n      ("33", "Movie 09", 5),\n      ("33", "Movie 10", 3),\n      ("44", "Movie 01", 4),\n      ("44", "Movie 02", 5),\n      ("44", "Movie 03", 1),\n      ("44", "Movie 05", 3),\n      ("44", "Movie 06", 4),\n      ("44", "Movie 07", 5),\n      ("44", "Movie 08", 1),\n      ("44", "Movie 10", 3))\n    .toDF("customerIDOrg", "itemIDOrg", "rating")\n    .dropDuplicates()\n    .cache())\n\nval recommendationIndexer = (new RecommendationIndexer()\n    .setUserInputCol("customerIDOrg")\n    .setUserOutputCol("customerID")\n    .setItemInputCol("itemIDOrg")\n    .setItemOutputCol("itemID")\n    .setRatingCol("rating"))\n\nval algo = (new SAR()\n      .setUserCol("customerID")\n      .setItemCol("itemID")\n      .setRatingCol("rating")\n      .setTimeCol("timestamp")\n      .setSupportThreshold(1)\n      .setSimilarityFunction("jacccard")\n      .setActivityTimeFormat("EEE MMM dd HH:mm:ss Z yyyy"))\n\nval adapter = (new RankingAdapter()\n      .setK(5)\n      .setRecommender(algo))\n\nval res1 = recommendationIndexer.fit(ratings).transform(ratings).cache()\n\ndisplay(adapter.fit(res1).transform(res1))\n')))),(0,r.kt)(l.Z,{className:"SAR",py:"mmlspark.recommendation.html#module-mmlspark.recommendation.SAR",scala:"com/microsoft/azure/synapse/ml/recommendation/SAR.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/recommendation/SAR.scala",mdxType:"DocTable"}))}y.isMDXComponent=!0},7435:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return p},toc:function(){return d},default:function(){return y}});var a=n(2122),o=n(9756),r=(n(7294),n(3905)),s=n(5386),i=n(1332),l=n(1989),m=["components"],c={},u=void 0,p={unversionedId:"documentation/estimators/core/_Stages",id:"version-0.9.1/documentation/estimators/core/_Stages",isDocsHomePage:!1,title:"_Stages",description:"\x3c!--",source:"@site/versioned_docs/version-0.9.1/documentation/estimators/core/_Stages.md",sourceDirName:"documentation/estimators/core",slug:"/documentation/estimators/core/_Stages",permalink:"/docs/documentation/estimators/core/_Stages",version:"0.9.1",frontMatter:{}},d=[{value:"Stages",id:"stages",children:[{value:"ClassBalancer",id:"classbalancer",children:[]},{value:"MultiColumnAdapter",id:"multicolumnadapter",children:[]},{value:"Timer",id:"timer",children:[]}]}],f={toc:d};function y(e){var t=e.components,n=(0,o.Z)(e,m);return(0,r.kt)("wrapper",(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"stages"},"Stages"),(0,r.kt)("h3",{id:"classbalancer"},"ClassBalancer"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.stages import *\n\ndf = (spark.createDataFrame([\n      (0, 1.0, "Hi I"),\n      (1, 1.0, "I wish for snow today"),\n      (2, 2.0, "I wish for snow today"),\n      (3, 2.0, "I wish for snow today"),\n      (4, 2.0, "I wish for snow today"),\n      (5, 2.0, "I wish for snow today"),\n      (6, 0.0, "I wish for snow today"),\n      (7, 1.0, "I wish for snow today"),\n      (8, 0.0, "we Cant go to the park, because of the snow!"),\n      (9, 2.0, "")\n      ], ["index", "label", "sentence"]))\n\ncb = ClassBalancer().setInputCol("label")\n\ndisplay(cb.fit(df).transform(df))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.stages._\n\nval df = Seq(\n      (0, 1.0, "Hi I"),\n      (1, 1.0, "I wish for snow today"),\n      (2, 2.0, "I wish for snow today"),\n      (3, 2.0, "I wish for snow today"),\n      (4, 2.0, "I wish for snow today"),\n      (5, 2.0, "I wish for snow today"),\n      (6, 0.0, "I wish for snow today"),\n      (7, 1.0, "I wish for snow today"),\n      (8, 0.0, "we Cant go to the park, because of the snow!"),\n      (9, 2.0, "")).toDF("index", "label", "sentence")\n\nval cb = new ClassBalancer().setInputCol("label")\n\ndisplay(cb.fit(df).transform(df))\n')))),(0,r.kt)(l.Z,{className:"ClassBalancer",py:"synapse.ml.stages.html#module-synapse.ml.stages.ClassBalancer",scala:"com/microsoft/azure/synapse/ml/stages/ClassBalancer.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/stages/ClassBalancer.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"multicolumnadapter"},"MultiColumnAdapter"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.stages import *\nfrom pyspark.ml.feature import Tokenizer\n\ndf = (spark.createDataFrame([\n        (0, "This is a test", "this is one too"),\n        (1, "could be a test", "bar"),\n        (2, "foo", "bar"),\n        (3, "foo", "maybe not")\n      ], ["label", "words1", "words2"]))\n\nstage1 = Tokenizer()\nmca = (MultiColumnAdapter()\n        .setBaseStage(stage1)\n        .setInputCols(["words1",  "words2"])\n        .setOutputCols(["output1", "output2"]))\n\ndisplay(mca.fit(df).transform(df))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.stages._\nimport org.apache.spark.ml.feature.Tokenizer\n\nval df = (Seq(\n    (0, "This is a test", "this is one too"),\n    (1, "could be a test", "bar"),\n    (2, "foo", "bar"),\n    (3, "foo", "maybe not"))\n    .toDF("label", "words1", "words2"))\n\nval stage1 = new Tokenizer()\nval mca = (new MultiColumnAdapter()\n        .setBaseStage(stage1)\n        .setInputCols(Array[String]("words1",  "words2"))\n        .setOutputCols(Array[String]("output1", "output2")))\n\ndisplay(mca.fit(df).transform(df))\n')))),(0,r.kt)(l.Z,{className:"MultiColumnAdapter",py:"synapse.ml.stages.html#module-synapse.ml.stages.MultiColumnAdapter",scala:"com/microsoft/azure/synapse/ml/stages/MultiColumnAdapter.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/stages/MultiColumnAdapter.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"timer"},"Timer"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.stages import *\nfrom pyspark.ml.feature import *\n\ndf = (spark.createDataFrame([\n        (0, "Hi I"),\n        (1, "I wish for snow today"),\n        (2, "we Cant go to the park, because of the snow!"),\n        (3, "")\n      ], ["label", "sentence"]))\n\ntok = (Tokenizer()\n      .setInputCol("sentence")\n      .setOutputCol("tokens"))\n\ndf2 = Timer().setStage(tok).fit(df).transform(df)\n\ndf3 = HashingTF().setInputCol("tokens").setOutputCol("hash").transform(df2)\n\nidf = IDF().setInputCol("hash").setOutputCol("idf")\ntimer = Timer().setStage(idf)\n\ndisplay(timer.fit(df3).transform(df3))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.stages._\nimport org.apache.spark.ml.feature._\n\nval df = (Seq(\n    (0, "Hi I"),\n    (1, "I wish for snow today"),\n    (2, "we Cant go to the park, because of the snow!"),\n    (3, "")\n  ).toDF("label", "sentence"))\n\nval tok = (new Tokenizer()\n      .setInputCol("sentence")\n      .setOutputCol("tokens"))\n\nval df2 = new Timer().setStage(tok).fit(df).transform(df)\n\nval df3 = new HashingTF().setInputCol("tokens").setOutputCol("hash").transform(df2)\n\nval idf = new IDF().setInputCol("hash").setOutputCol("idf")\nval timer = new Timer().setStage(idf)\n\ndisplay(timer.fit(df3).transform(df3))\n')))),(0,r.kt)(l.Z,{className:"Timer",py:"synapse.ml.stages.html#module-synapse.ml.stages.Timer",scala:"com/microsoft/azure/synapse/ml/stages/Timer.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/stages/Timer.scala",mdxType:"DocTable"}))}y.isMDXComponent=!0},6967:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return c},contentTitle:function(){return u},metadata:function(){return p},toc:function(){return d},default:function(){return y}});var a=n(2122),o=n(9756),r=(n(7294),n(3905)),s=n(5386),i=n(1332),l=n(1989),m=["components"],c={},u=void 0,p={unversionedId:"documentation/estimators/core/_Train",id:"version-0.9.1/documentation/estimators/core/_Train",isDocsHomePage:!1,title:"_Train",description:"\x3c!--",source:"@site/versioned_docs/version-0.9.1/documentation/estimators/core/_Train.md",sourceDirName:"documentation/estimators/core",slug:"/documentation/estimators/core/_Train",permalink:"/docs/documentation/estimators/core/_Train",version:"0.9.1",frontMatter:{}},d=[{value:"Train",id:"train",children:[{value:"TrainClassifier",id:"trainclassifier",children:[]},{value:"TrainRegressor",id:"trainregressor",children:[]}]}],f={toc:d};function y(e){var t=e.components,n=(0,o.Z)(e,m);return(0,r.kt)("wrapper",(0,a.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"train"},"Train"),(0,r.kt)("h3",{id:"trainclassifier"},"TrainClassifier"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.train import *\nfrom pyspark.ml.classification import LogisticRegression\n\ndf = spark.createDataFrame([\n      (0, 2, 0.50, 0.60, 0),\n      (1, 3, 0.40, 0.50, 1),\n      (0, 4, 0.78, 0.99, 2),\n      (1, 5, 0.12, 0.34, 3),\n      (0, 1, 0.50, 0.60, 0),\n      (1, 3, 0.40, 0.50, 1),\n      (0, 3, 0.78, 0.99, 2),\n      (1, 4, 0.12, 0.34, 3),\n      (0, 0, 0.50, 0.60, 0),\n      (1, 2, 0.40, 0.50, 1),\n      (0, 3, 0.78, 0.99, 2),\n      (1, 4, 0.12, 0.34, 3)],\n      ["Label", "col1", "col2", "col3", "col4"]\n)\n\ntc = (TrainClassifier()\n      .setModel(LogisticRegression())\n      .setLabelCol("Label"))\n\ndisplay(tc.fit(df).transform(df))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.train._\nimport org.apache.spark.ml.classification.LogisticRegression\n\nval df = (Seq(\n      (0, 2, 0.50, 0.60, 0),\n      (1, 3, 0.40, 0.50, 1),\n      (0, 4, 0.78, 0.99, 2),\n      (1, 5, 0.12, 0.34, 3),\n      (0, 1, 0.50, 0.60, 0),\n      (1, 3, 0.40, 0.50, 1),\n      (0, 3, 0.78, 0.99, 2),\n      (1, 4, 0.12, 0.34, 3),\n      (0, 0, 0.50, 0.60, 0),\n      (1, 2, 0.40, 0.50, 1),\n      (0, 3, 0.78, 0.99, 2),\n      (1, 4, 0.12, 0.34, 3))\n      .toDF("Label", "col1", "col2", "col3", "col4"))\n\nval tc = (new TrainClassifier()\n      .setModel(new LogisticRegression())\n      .setLabelCol("Label"))\n\ndisplay(tc.fit(df).transform(df))\n')))),(0,r.kt)(l.Z,{className:"TrainClassifier",py:"mmlspark.train.html#module-mmlspark.train.TrainClassifier",scala:"com/microsoft/azure/synapse/ml/train/TrainClassifier.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/train/TrainClassifier.scala",mdxType:"DocTable"}),(0,r.kt)("h3",{id:"trainregressor"},"TrainRegressor"),(0,r.kt)(s.Z,{defaultValue:"py",values:[{label:"Python",value:"py"},{label:"Scala",value:"scala"}],mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"py",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.train import *\nfrom pyspark.ml.classification import LogisticRegression\n\ndataset = (spark.createDataFrame([\n    (0.0, 2, 0.50, 0.60, 0.0),\n    (1.0, 3, 0.40, 0.50, 1.0),\n    (2.0, 4, 0.78, 0.99, 2.0),\n    (3.0, 5, 0.12, 0.34, 3.0),\n    (0.0, 1, 0.50, 0.60, 0.0),\n    (1.0, 3, 0.40, 0.50, 1.0),\n    (2.0, 3, 0.78, 0.99, 2.0),\n    (3.0, 4, 0.12, 0.34, 3.0),\n    (0.0, 0, 0.50, 0.60, 0.0),\n    (1.0, 2, 0.40, 0.50, 1.0),\n    (2.0, 3, 0.78, 0.99, 2.0),\n    (3.0, 4, 0.12, 0.34, 3.0)],\n    ["label", "col1", "col2", "col3", "prediction"]))\n\nlinearRegressor = (LinearRegression()\n      .setRegParam(0.3)\n      .setElasticNetParam(0.8))\ntrainRegressor = (TrainRegressor()\n      .setModel(linearRegressor)\n      .setLabelCol("Label"))\n\ndisplay(trainRegressor.fit(dataset).transform(dataset))\n'))),(0,r.kt)(i.Z,{value:"scala",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import com.microsoft.azure.synapse.ml.train._\nimport org.apache.spark.ml.classification.LogisticRegression\n\nval dataset = spark.createDataFrame(Seq(\n    (0.0, 2, 0.50, 0.60, 0.0),\n    (1.0, 3, 0.40, 0.50, 1.0),\n    (2.0, 4, 0.78, 0.99, 2.0),\n    (3.0, 5, 0.12, 0.34, 3.0),\n    (0.0, 1, 0.50, 0.60, 0.0),\n    (1.0, 3, 0.40, 0.50, 1.0),\n    (2.0, 3, 0.78, 0.99, 2.0),\n    (3.0, 4, 0.12, 0.34, 3.0),\n    (0.0, 0, 0.50, 0.60, 0.0),\n    (1.0, 2, 0.40, 0.50, 1.0),\n    (2.0, 3, 0.78, 0.99, 2.0),\n    (3.0, 4, 0.12, 0.34, 3.0)))\n    .toDF("label", "col1", "col2", "col3", "prediction")\n\nval linearRegressor = (new LinearRegression()\n      .setRegParam(0.3)\n      .setElasticNetParam(0.8))\nval trainRegressor = (new TrainRegressor()\n      .setModel(linearRegressor)\n      .setLabelCol("Label"))\n\ndisplay(trainRegressor.fit(dataset).transform(dataset))\n')))),(0,r.kt)(l.Z,{className:"TrainRegressor",py:"mmlspark.train.html#module-mmlspark.train.TrainRegressor",scala:"com/microsoft/azure/synapse/ml/train/TrainRegressor.html",sourceLink:"https://github.com/microsoft/SynapseML/blob/master/core/src/main/scala/com/microsoft/azure/synapse/ml/train/TrainRegressor.scala",mdxType:"DocTable"}))}y.isMDXComponent=!0},4741:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return f},contentTitle:function(){return y},metadata:function(){return v},toc:function(){return g},default:function(){return b}});var a=n(2122),o=n(9756),r=(n(7294),n(3905)),s=n(5207),i=n(8830),l=n(2569),m=n(6741),c=n(1522),u=n(7435),p=n(6967),d=["components"],f={title:"Estimators - Core",sidebar_label:"Core",hide_title:!0},y=void 0,v={unversionedId:"documentation/estimators/estimators_core",id:"version-0.9.1/documentation/estimators/estimators_core",isDocsHomePage:!1,title:"Estimators - Core",description:"export const toc = [...AutoMLTOC, ...FeaturizeTOC, ...IsolationForestTOC,",source:"@site/versioned_docs/version-0.9.1/documentation/estimators/estimators_core.md",sourceDirName:"documentation/estimators",slug:"/documentation/estimators/estimators_core",permalink:"/docs/documentation/estimators/estimators_core",version:"0.9.1",frontMatter:{title:"Estimators - Core",sidebar_label:"Core",hide_title:!0},sidebar:"version-0.9.1/docs",previous:{title:"Deep Learning",permalink:"/docs/documentation/transformers/transformers_deep_learning"},next:{title:"LightGBM",permalink:"/docs/documentation/estimators/estimators_lightgbm"}},g=[].concat(s.toc,i.toc,l.toc,m.toc,c.toc,u.toc,p.toc),k={toc:g};function b(e){var t=e.components,n=(0,o.Z)(e,d);return(0,r.kt)("wrapper",(0,a.Z)({},k,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)(s.default,{mdxType:"AutoML"}),(0,r.kt)(i.default,{mdxType:"Featurize"}),(0,r.kt)(l.default,{mdxType:"IsolationForest"}),(0,r.kt)(m.default,{mdxType:"NN"}),(0,r.kt)(c.default,{mdxType:"Recommendation"}),(0,r.kt)(u.default,{mdxType:"Stages"}),(0,r.kt)(p.default,{mdxType:"Train"}))}b.isMDXComponent=!0},6010:function(e,t,n){"use strict";function a(e){var t,n,o="";if("string"==typeof e||"number"==typeof e)o+=e;else if("object"==typeof e)if(Array.isArray(e))for(t=0;t<e.length;t++)e[t]&&(n=a(e[t]))&&(o&&(o+=" "),o+=n);else for(t in e)e[t]&&(o&&(o+=" "),o+=t);return o}function o(){for(var e,t,n=0,o="";n<arguments.length;)(e=arguments[n++])&&(t=a(e))&&(o&&(o+=" "),o+=t);return o}n.d(t,{Z:function(){return o}})}}]);