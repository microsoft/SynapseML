"use strict";(self.webpackChunksynapseml=self.webpackChunksynapseml||[]).push([[2147],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>d});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},m=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},c=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),c=p(n),d=o,h=c["".concat(l,".").concat(d)]||c[d]||u[d]||a;return n?r.createElement(h,s(s({ref:t},m),{},{components:n})):r.createElement(h,s({ref:t},m))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,s=new Array(a);s[0]=c;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,s[1]=i;for(var p=2;p<a;p++)s[p]=n[p];return r.createElement.apply(null,s)}return r.createElement.apply(null,n)}c.displayName="MDXCreateElement"},18306:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>p});var r=n(83117),o=(n(67294),n(3905));const a={title:"Quickstart - Multimodal OpenAI Prompter with Responses API",hide_title:!0,status:"stable"},s="Quickstart: Multimodal OpenAI Prompter with Responses API",i={unversionedId:"Explore Algorithms/OpenAI/Quickstart - Multimodal OpenAI Prompter with Responses API",id:"Explore Algorithms/OpenAI/Quickstart - Multimodal OpenAI Prompter with Responses API",title:"Quickstart - Multimodal OpenAI Prompter with Responses API",description:"Use OpenAI's Responses API to enrich your data with multimodal content such as PDFs, images or documentations. This quickstart shows how to use multimodal content from a Spark DataFrame by pointing OpenAIPrompt to columns that contain file paths or URLs.",source:"@site/docs/Explore Algorithms/OpenAI/Quickstart - Multimodal OpenAI Prompter with Responses API.md",sourceDirName:"Explore Algorithms/OpenAI",slug:"/Explore Algorithms/OpenAI/Quickstart - Multimodal OpenAI Prompter with Responses API",permalink:"/SynapseML/docs/next/Explore Algorithms/OpenAI/Quickstart - Multimodal OpenAI Prompter with Responses API",draft:!1,tags:[],version:"current",frontMatter:{title:"Quickstart - Multimodal OpenAI Prompter with Responses API",hide_title:!0,status:"stable"}},l={},p=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Fill in service information",id:"fill-in-service-information",level:2},{value:"Build a multimodal dataset",id:"build-a-multimodal-dataset",level:2},{value:"Configure <code>OpenAIPrompt</code> for file-aware responses",id:"configure-openaiprompt-for-file-aware-responses",level:2},{value:"Generate multimodal responses",id:"generate-multimodal-responses",level:2}],m={toc:p};function u(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"quickstart-multimodal-openai-prompter-with-responses-api"},"Quickstart: Multimodal OpenAI Prompter with Responses API"),(0,o.kt)("p",null,"Use OpenAI's Responses API to enrich your data with multimodal content such as PDFs, images or documentations. This quickstart shows how to use multimodal content from a Spark DataFrame by pointing ",(0,o.kt)("inlineCode",{parentName:"p"},"OpenAIPrompt")," to columns that contain file paths or URLs."),(0,o.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"An active Spark session (",(0,o.kt)("inlineCode",{parentName:"li"},"spark"),")"),(0,o.kt)("li",{parentName:"ul"},"Access to an OpenAI resource with the Responses API enabled. Deployment should be a multimodal model."),(0,o.kt)("li",{parentName:"ul"},"File you want to talk to.")),(0,o.kt)("h2",{id:"fill-in-service-information"},"Fill in service information"),(0,o.kt)("p",null,"Next, edit the cell in the notebook to point to your service. In particular set the ",(0,o.kt)("inlineCode",{parentName:"p"},"service_name"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"deployment_name"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"location"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"key")," variables to match them to your OpenAI service:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.core.platform import find_secret\n\n# Fill in the following lines with your service information\n# Learn more about selecting which embedding model to choose: https://openai.com/blog/new-and-improved-embedding-model\nservice_name = "synapseml-openai-2"\ndeployment_name = "gpt-4.1"\napi_version = (\n    "2025-04-01-preview"  # Responses API is only supported in this version and later\n)\n\nkey = find_secret(\n    secret_name="openai-api-key-2", keyvault="mmlspark-build-keys"\n)  # please replace this line with your key as a string\n\nassert key is not None and service_name is not None\n')),(0,o.kt)("h2",{id:"build-a-multimodal-dataset"},"Build a multimodal dataset"),(0,o.kt)("p",null,"Each row combines a natural-language question with a reference to a file. Attachments can be local paths or remote URLs(Currently only support http(s))."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'qa_df = spark.createDataFrame(\n    [\n        (\n            "What\'s in the image?",\n            "https://mmlspark.blob.core.windows.net/datasets/OCR/test2.png",\n        ),\n        (\n            "Summarize this document.",\n            "https://mmlspark.blob.core.windows.net/datasets/OCR/paper.pdf",\n        ),\n        (\n            "What are the related works to SynapseML?",\n            "https://mmlspark.blob.core.windows.net/publicwasb/Overview.md",\n        ),\n    ]\n).toDF("questions", "urls")\n')),(0,o.kt)("h2",{id:"configure-openaiprompt-for-file-aware-responses"},"Configure ",(0,o.kt)("inlineCode",{parentName:"h2"},"OpenAIPrompt")," for file-aware responses"),(0,o.kt)("p",null,"Any column declared as ",(0,o.kt)("inlineCode",{parentName:"p"},"path")," is treated as an attachment. The prompt template automatically replaces ",(0,o.kt)("inlineCode",{parentName:"p"},"{file_path}")," with a notice that the model will receive the referenced file in the message payload."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from synapse.ml.services.openai import OpenAIPrompt\n\nprompt_template = "{questions}: {urls}"\n\nprompter = (\n    OpenAIPrompt()\n    .setSubscriptionKey(key)\n    .setDeploymentName(deployment_name)\n    .setCustomServiceName(service_name)\n    .setApiVersion(api_version)\n    .setApiType(\n        "responses"\n    )  # We need to use the Responses API, instead of Chat Completions API.\n    .setPromptTemplate(prompt_template)\n    .setColumnTypes(\n        {"urls": "path"}\n    )  # Configure the column type to \'path\' for the url column\n    .setErrorCol("error")\n    .setOutputCol("outputCol")\n)\n')),(0,o.kt)("h2",{id:"generate-multimodal-responses"},"Generate multimodal responses"),(0,o.kt)("p",null,"See how the model react to your various file data."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"prompter.transform(qa_df).show(truncate=False)\n")))}u.isMDXComponent=!0}}]);