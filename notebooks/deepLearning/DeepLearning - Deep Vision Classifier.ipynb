{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning - Deep Vision Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup -- reinstall horovod based on new version of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install /dbfs/FileStore/shared_uploads/serenaruan@microsoft.com/synapseml_dl-0.9.5.dev1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# Remove Outdated Signing Key:\n",
    "sudo apt-key del 7fa2af80\n",
    "\n",
    "# Install the new cuda-keyring package:\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb\n",
    "sudo dpkg -i cuda-keyring_1.0-1_all.deb\n",
    "\n",
    "apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64/7fa2af80.pub\n",
    "wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64/nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\n",
    "dpkg -i ./nvidia-machine-learning-repo-ubuntu2004_1.0.0-1_amd64.deb\n",
    "\n",
    "\n",
    "apt-get update\n",
    "apt-get install --allow-downgrades --no-install-recommends -y \\\n",
    "cuda-nvml-dev-11-0=11.0.167-1 \\\n",
    "cuda-nvcc-11-0=11.0.221-1 \\\n",
    "cuda-cudart-dev-11-0=11.0.221-1 \\\n",
    "cuda-libraries-dev-11-0=11.0.3-1 \\\n",
    "libnccl-dev=2.10.3-1+cuda11.0 \\\n",
    "libcusparse-dev-11-0=11.1.1.245-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "git clone --recursive https://github.com/horovod/horovod.git\n",
    "cd horovod\n",
    "# fix a certain commit at version 0.24.3\n",
    "git reset --hard 7707267a4bef79e09a9df1d41b0652feb61b76c7\n",
    "rm -rf build/ dist/\n",
    "HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_CUDA_HOME=/usr/local/cuda-11/ HOROVOD_WITH_PYTORCH=1 HOROVOD_WITHOUT_MXNET=1 \\\n",
    "/databricks/python3/bin/python setup.py bdist_wheel\n",
    "\n",
    "readlink -f dist/horovod-*.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --no-cache-dir /databricks/driver/horovod/dist/horovod-0.24.3-cp38-cp38-linux_x86_64.whl --force-reinstall --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! horovodrun --check-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types as T\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these files already exist for internal build test machine\n",
    "train_files = [\n",
    "    os.path.join(dp, f)\n",
    "    for dp, dn, filenames in os.walk(\"/dbfs/tmp/17flowers/train\")\n",
    "    for f in filenames\n",
    "    if os.path.splitext(f)[1] == \".jpg\"\n",
    "]\n",
    "test_files = [\n",
    "    os.path.join(dp, f)\n",
    "    for dp, dn, filenames in os.walk(\"/dbfs/tmp/17flowers/test\")\n",
    "    for f in filenames\n",
    "    if os.path.splitext(f)[1] == \".jpg\"\n",
    "]\n",
    "\n",
    "\n",
    "def extract_path_and_label(path):\n",
    "    num = int(path.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1])\n",
    "    label = num // 81  # Assign the label\n",
    "    return (path, label)\n",
    "\n",
    "\n",
    "train_df = spark.createDataFrame(\n",
    "    map(extract_path_and_label, train_files), [\"image\", \"label\"]\n",
    ").withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
    "\n",
    "test_df = spark.createDataFrame(\n",
    "    map(extract_path_and_label, test_files), [\"image\", \"label\"]\n",
    ").withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
    "\n",
    "display(train_df.limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def _transform_row(row):\n",
    "    image = Image.open(row[\"image\"]).convert(\"RGB\")\n",
    "    image = transform(image).numpy()\n",
    "    label = row[\"label\"]\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "\n",
    "def readImageAndTransform(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    image = DenseVector(transform(image).numpy().reshape(-1))\n",
    "    return image\n",
    "\n",
    "\n",
    "read_image_and_transform_udf = udf(lambda x: readImageAndTransform(x), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from horovod.spark.common.store import DBFSLocalStore\n",
    "from horovod.spark.common.backend import SparkBackend\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from synapse.ml.dl import *\n",
    "\n",
    "run_output_dir = \"/dbfs/FileStore/test/resnet50\"\n",
    "store = DBFSLocalStore(run_output_dir)\n",
    "\n",
    "backend = SparkBackend(\n",
    "    num_proc=2,  # This is important parameter\n",
    "    stdout=sys.stdout,\n",
    "    stderr=sys.stderr,\n",
    "    prefix_output_with_timestamp=True,\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "class MyDummyCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.epcoh_end_counter = 0\n",
    "        self.train_epcoh_end_counter = 0\n",
    "        self.validation_epoch_end_counter = 0\n",
    "\n",
    "    def on_init_start(self, trainer):\n",
    "        print(\"Starting to init trainer!\")\n",
    "\n",
    "    def on_init_end(self, trainer):\n",
    "        print(\"Trainer is initialized.\")\n",
    "\n",
    "    def on_epoch_end(self, trainer, model):\n",
    "        print(\"A epoch ended.\")\n",
    "        self.epcoh_end_counter += 1\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, model, unused=None):\n",
    "        print(\"A train epoch ended.\")\n",
    "        self.train_epcoh_end_counter += 1\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, model, unused=None):\n",
    "        print(\"A val epoch ended.\")\n",
    "        self.validation_epoch_end_counter += 1\n",
    "\n",
    "    def on_train_end(self, trainer, model):\n",
    "        print(\n",
    "            \"Training ends:\"\n",
    "            f\"epcoh_end_counter={self.epcoh_end_counter}, \"\n",
    "            f\"train_epcoh_end_counter={self.train_epcoh_end_counter}, \"\n",
    "            f\"validation_epoch_end_counter={self.validation_epoch_end_counter} \\n\"\n",
    "        )\n",
    "        assert self.train_epcoh_end_counter <= epochs\n",
    "        assert (\n",
    "            self.train_epcoh_end_counter + self.validation_epoch_end_counter\n",
    "            == self.epcoh_end_counter\n",
    "        )\n",
    "\n",
    "\n",
    "callbacks = [MyDummyCallback(), ModelCheckpoint(filename=\"{epoch}-{train_loss:.2f}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_vision_classifier = DeepVisionClassifier(\n",
    "    backbone=\"resnet50\",\n",
    "    store=store,\n",
    "    backend=backend,\n",
    "    callbacks=callbacks,\n",
    "    input_shapes=[[-1, 3, 224, 224]],\n",
    "    num_classes=17,\n",
    "    feature_cols=[\"image\"],\n",
    "    label_cols=[\"label\"],\n",
    "    batch_size=16,\n",
    "    epochs=epochs,\n",
    "    validation=0.1,\n",
    "    verbose=1,\n",
    "    profiler=None,\n",
    "    partitions_per_process=1,\n",
    "    transformation_fn=_transform_row,\n",
    ")\n",
    "\n",
    "deep_vision_model = deep_vision_classifier.fit(train_df).setOutputCols([\"label_prob\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_trans = test_df.withColumn(\"features\", read_image_and_transform_udf(\"image\"))\n",
    "pred_df = deep_vision_model.setFeatureColumns([\"features\"]).transform(test_df_trans)\n",
    "argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\n",
    "pred_df = pred_df.withColumn(\"label_pred\", argmax(pred_df.label_prob))\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"label_pred\", labelCol=\"label\", metricName=\"accuracy\"\n",
    ")\n",
    "print(\"Test accuracy:\", evaluator.evaluate(pred_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('synapseml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e618ed4c1ce23faf0894af85b02b5324888e2d70eeb0f2451cf171faa1cbba7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
