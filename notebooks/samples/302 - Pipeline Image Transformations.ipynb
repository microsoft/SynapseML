{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 302 - Pipeline Image Transformation with OpenCV\n",
    "\n",
    "This example shows how to manipulate the collection of images.\n",
    "First, the images are downloaded to the local directory.\n",
    "Second, they are copied to your cluster's attached HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mml-deploy": "local",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"datasets/CIFAR10\"\n",
    "\n",
    "import os, subprocess\n",
    "from urllib.request import urlretrieve\n",
    "dataFile = \"test.zip\"\n",
    "if not os.path.isdir(IMAGE_PATH):\n",
    "    os.makedirs(IMAGE_PATH)\n",
    "    urlretrieve(\"https://mmlspark.azureedge.net/datasets/CIFAR10/test.zip\",\n",
    "                IMAGE_PATH + \".zip\")\n",
    "    print(subprocess.check_output(\n",
    "              \"ip=\\\"%s\\\"; cd \\\"$ip\\\" && unzip -q \\\"../$(basename $PWD).zip\\\"\" % IMAGE_PATH,\n",
    "              stderr = subprocess.STDOUT, shell = True)\n",
    "          .decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mml-deploy": "hdinsight",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "IMAGE_PATH = \"/datasets/CIFAR10/test\"\n",
    "import subprocess\n",
    "if subprocess.call([\"hdfs\", \"dfs\", \"-test\", \"-d\", IMAGE_PATH]):\n",
    "    from urllib import urlretrieve\n",
    "    urlretrieve(\"https://mmlspark.azureedge.net/datasets/CIFAR10/test.zip\", \"/tmp/test.zip\")\n",
    "    print subprocess.check_output(\"rm -rf /tmp/CIFAR10 && mkdir -p /tmp/CIFAR10 && unzip /tmp/test.zip -d /tmp/CIFAR10\", stderr=subprocess.STDOUT, shell=True)\n",
    "    print subprocess.check_output(\"hdfs dfs -mkdir -p %s\" % IMAGE_PATH, stderr=subprocess.STDOUT, shell=True)\n",
    "    print subprocess.check_output(\"hdfs dfs -copyFromLocal -f /tmp/CIFAR10/test/011*.png %s\"%IMAGE_PATH, stderr=subprocess.STDOUT, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mml-deploy": "hdinsight",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"/datasets/CIFAR10/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are loaded from the directory (for fast prototyping, consider loading a fraction of\n",
    "images). Inside the dataframe, each image is a single field in the image column. The image has\n",
    "sub-fields (path, height, width, OpenCV type and OpenCV bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mmlspark\n",
    "import numpy as np\n",
    "from mmlspark import toNDArray\n",
    "\n",
    "images = spark.readImages(IMAGE_PATH, recursive = True, sampleRatio = 0.1).cache()\n",
    "images.printSchema()\n",
    "print(images.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When collected from the *DataFrame*, the image data are stored in a *Row*, which is Spark's way\n",
    "to represent structures (in the current example, each dataframe row has a single Image, which\n",
    "itself is a Row).  It is possible to address image fields by name and use `toNDArray()` helper\n",
    "function to convert the image into numpy array for further manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "data = images.take(3)    # take first three rows of the dataframe\n",
    "im = data[2][0]          # the image is in the first column of a given row\n",
    "\n",
    "print(\"image type: {}, number of fields: {}\".format(type(im), len(im)))\n",
    "print(\"image path: {}\".format(im.path))\n",
    "print(\"height: {}, width: {}, OpenCV type: {}\".format(im.height, im.width, im.type))\n",
    "\n",
    "arr = toNDArray(im)     # convert to numpy array\n",
    "Image.fromarray(arr, \"RGB\")   # display the image inside notebook\n",
    "print(images.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ImageTransformer` for the basic image manipulation: resizing, cropping, etc.\n",
    "Internally, operations are pipelined and backed by OpenCV implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mmlspark import ImageTransformer\n",
    "\n",
    "tr = (ImageTransformer()                  # images are resized and then cropped\n",
    "      .setOutputCol(\"transformed\")\n",
    "      .resize(height = 200, width = 200)\n",
    "      .crop(0, 0, height = 180, width = 180) )\n",
    "\n",
    "small = tr.transform(images).select(\"transformed\")\n",
    "\n",
    "im = small.take(3)[2][0]                  # take third image\n",
    "Image.fromarray(toNDArray(im), \"RGB\")   # display the image inside notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the advanced image manipulations, use Spark UDFs.\n",
    "The MMLSpark package provides conversion function between *Spark Row* and\n",
    "*ndarray* image representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from mmlspark import ImageSchema, toNDArray, toImage\n",
    "\n",
    "def u(row):\n",
    "    array = toNDArray(row)    # convert Image to numpy ndarray[height, width, 3]\n",
    "    array[:,:,2] = 0\n",
    "    return toImage(array)     # numpy array back to Spark Row structure\n",
    "\n",
    "noBlueUDF = udf(u,ImageSchema)\n",
    "\n",
    "noblue = small.withColumn(\"noblue\", noBlueUDF(small[\"transformed\"])).select(\"noblue\")\n",
    "\n",
    "im = noblue.take(3)[2][0]                # take second image\n",
    "Image.fromarray(toNDArray(im), \"RGB\")   # display the image inside notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images could be unrolled into the dense 1D vectors suitable for CNTK evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mmlspark import UnrollImage\n",
    "\n",
    "unroller = UnrollImage().setInputCol(\"noblue\").setOutputCol(\"unrolled\")\n",
    "\n",
    "unrolled = unroller.transform(noblue).select(\"unrolled\")\n",
    "\n",
    "vector = unrolled.take(1)[0][0]\n",
    "print(type(vector))\n",
    "len(vector.toArray())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
