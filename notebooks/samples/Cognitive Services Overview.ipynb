{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<img width=\"200\"  src=\"https://mmlspark.blob.core.windows.net/graphics/Readme/cog_services_on_spark_2.svg\">\n",
    "\n",
    "# Cognitive Services\n",
    "\n",
    "[Azure Cognitive Services](https://azure.microsoft.com/en-us/services/cognitive-services/) are a suite of APIs, SDKs, and services available to help developers build intelligent applications without having direct AI or data science skills or knowledge by enabling developers to easily add cognitive features into their applications. The goal of Azure Cognitive Services is to help developers create applications that can see, hear, speak, understand, and even begin to reason. The catalog of services within Azure Cognitive Services can be categorized into five main pillars - Vision, Speech, Language, Web Search, and Decision.\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Vision\n",
    "[**Computer Vision**](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/)\n",
    "- Describe: provides description of an image in human readable language ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/DescribeImage.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.DescribeImage))\n",
    "- Analyze (color, image type, face, adult/racy content): analyzes visual features of an image ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/AnalyzeImage.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.AnalyzeImage))\n",
    "- OCR: reads text from an image ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/OCR.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.OCR))\n",
    "- Recognize Text: reads text from an image ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/RecognizeText.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.RecognizeText))\n",
    "- Thumbnail: generates a thumbnail of user-specified size from the image ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/GenerateThumbnails.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.GenerateThumbnails))\n",
    "- Recognize domain-specific content: recognizes domain-specific content (celebrity, landmark) ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/RecognizeDomainSpecificContent.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.RecognizeDomainSpecificContent))\n",
    "- Tag: identifies list of words that are relevant to the in0put image ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/TagImage.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.TagImage))\n",
    "\n",
    "[**Face**](https://azure.microsoft.com/en-us/services/cognitive-services/face/)\n",
    "- Detect: detects human faces in an image ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/DetectFace.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.DetectFace))\n",
    "- Verify: verifies whether two faces belong to a same person, or a face belongs to a person ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/VerifyFaces.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.VerifyFaces))\n",
    "- Identify: finds the closest matches of the specific query person face from a person group ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/IdentifyFaces.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.IdentifyFaces))\n",
    "- Find similar: finds similar faces to the query face in a face list ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/FindSimilarFace.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.FindSimilarFace))\n",
    "- Group: divides a group of faces into disjoint groups based on similarity ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/GroupFaces.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.GroupFaces))\n",
    "\n",
    "### Speech\n",
    "[**Speech Services**](https://azure.microsoft.com/en-us/services/cognitive-services/speech-services/)\n",
    "- Speech-to-text: transcribes audio streams ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/SpeechToText.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.SpeechToText))\n",
    "\n",
    "### Language\n",
    "[**Text Analytics**](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/)\n",
    "- Language detection: detects language of the input text ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/LanguageDetector.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.LanguageDetector))\n",
    "- Key phrase extraction: identifies the key talking points in the input text ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/KeyPhraseExtractor.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.KeyPhraseExtractor))\n",
    "- Named entity recognition: identifies known entities and general named entities in the input text ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/NER.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.NER))\n",
    "- Sentiment analysis: returns a score betwee 0 and 1 indicating the sentiment in the input text ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/TextSentiment.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.TextSentiment))\n",
    "\n",
    "### Decision\n",
    "[**Anomaly Detector**](https://azure.microsoft.com/en-us/services/cognitive-services/anomaly-detector/)\n",
    "- Anomaly status of latest point: generates a model using preceding points and determines whether the latest point is anomalous ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/DetectLastAnomaly.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.DetectLastAnomaly))\n",
    "- Find anomalies: generates a model using an entire series and finds anomalies in the series ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/DetectAnomalies.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.DetectAnomalies))\n",
    "\n",
    "### Web Search\n",
    "- [Bing Image search](https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/) ([Scala](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/scala/com/microsoft/ml/spark/cognitive/BingImageSearch.html), [Python](https://mmlspark.blob.core.windows.net/docs/1.0.0-rc3/pyspark/mmlspark.cognitive.html#module-mmlspark.cognitive.BingImageSearch))\n",
    "- [Azure Cognitive search](https://docs.microsoft.com/en-us/azure/search/search-what-is-azure-search)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Follow the steps in [Getting started](https://docs.microsoft.com/en-us/azure/cognitive-services/big-data/getting-started) to set up your Azure Databricks and Cognitive Services environment. This tutorial shows you how to install MMLSpark and how to create your Spark cluster in Databricks.\n",
    "1. After you create a new notebook in Azure Databricks, copy the **Shared code** below and paste into a new cell in your notebook.\n",
    "1. Choose a service sample, below, and copy paste it into a second new cell in your notebook.\n",
    "1. Replace any of the service subscription key placeholders with your own key.\n",
    "1. Choose the run button (triangle icon) in the upper right corner of the cell, then select **Run Cell**.\n",
    "1. View results in a table below the cell."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Shared code\n",
    "\n",
    "To get started, we'll need to add this code to the project:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark.cognitive import *\n",
    "import os\n",
    "\n",
    "# A general Cognitive Services key for Text Analytics and Computer Vision (or use separate keys that belong to each service)\n",
    "service_key = os.environ[\"TEXT_API_KEY\"]\n",
    "# A Bing Search v7 subscription key\n",
    "bing_search_key = os.environ[\"BING_IMAGE_SEARCH_KEY\"]\n",
    "# An Anomaly Dectector subscription key\n",
    "anomaly_key = os.environ[\"ANOMALY_API_KEY\"]"
   ]
  },
  {
   "source": [
    "## Text Analytics sample\n",
    "\n",
    "The [Text Analytics](../text-analytics/index.yml) service provides several algorithms for extracting intelligent insights from text. For example, we can find the sentiment of given input text. The service will return a score between 0.0 and 1.0 where low scores indicate negative sentiment and high score indicates positive sentiment.  This sample uses three simple sentences and returns the sentiment for each."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a dataframe that's tied to it's column names\n",
    "df = spark.createDataFrame([\n",
    "  (\"I am so happy today, its sunny!\", \"en-US\"),\n",
    "  (\"I am frustrated by this rush hour traffic\", \"en-US\"),\n",
    "  (\"The cognitive services on spark aint bad\", \"en-US\"),\n",
    "], [\"text\", \"language\"])\n",
    "\n",
    "# Run the Text Analytics service with options\n",
    "sentiment = (TextSentiment()\n",
    "    .setTextCol(\"text\")\n",
    "    .setLocation(\"eastus\")\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setOutputCol(\"sentiment\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setLanguageCol(\"language\"))\n",
    "\n",
    "# Show the results of your text query in a table format\n",
    "display(sentiment.transform(df).select(\"text\", col(\"sentiment\")[0].getItem(\"sentiment\").alias(\"sentiment\")))"
   ]
  },
  {
   "source": [
    "## Computer Vision sample\n",
    "\n",
    "[Computer Vision](../computer-vision/index.yml) analyzes images to identify structure such as faces, objects, and natural-language descriptions. In this sample, we tag a list of images. Tags are one-word descriptions of things in the image like recognizable objects, people, scenery, and actions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the image URLs\n",
    "df = spark.createDataFrame([\n",
    "        (\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/objects.jpg\", ),\n",
    "        (\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/dog.jpg\", ),\n",
    "        (\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/house.jpg\", )\n",
    "    ], [\"image\", ])\n",
    "\n",
    "# Run the Computer Vision service. Analyze Image extracts infortmation from/about the images.\n",
    "analysis = (AnalyzeImage()\n",
    "    .setLocation(\"eastus\")\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setVisualFeatures([\"Categories\",\"Color\",\"Description\",\"Faces\",\"Objects\",\"Tags\"])\n",
    "    .setOutputCol(\"analysis_results\")\n",
    "    .setImageUrlCol(\"image\")\n",
    "    .setErrorCol(\"error\"))\n",
    "\n",
    "# Show the results of what you wanted to pull out of the images.\n",
    "display(analysis.transform(df).select(\"image\", \"analysis_results.description.tags\"))"
   ]
  },
  {
   "source": [
    "## Bing Image Search sample\n",
    "\n",
    "[Bing Image Search](../bing-image-search/overview.md) searches the web to retrieve images related to a user's natural language query. In this sample, we use a text query that looks for images with quotes. It returns a list of image URLs that contain photos related to our query."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Number of images Bing will return per query\n",
    "imgsPerBatch = 10\n",
    "# A list of offsets, used to page into the search results\n",
    "offsets = [(i*imgsPerBatch,) for i in range(100)]\n",
    "# Since web content is our data, we create a dataframe with options on that data: offsets\n",
    "bingParameters = spark.createDataFrame(offsets, [\"offset\"])\n",
    "\n",
    "# Run the Bing Image Search service with our text query\n",
    "bingSearch = (BingImageSearch()\n",
    "    .setSubscriptionKey(bing_search_key)\n",
    "    .setOffsetCol(\"offset\")\n",
    "    .setQuery(\"Martin Luther King Jr. quotes\")\n",
    "    .setCount(imgsPerBatch)\n",
    "    .setOutputCol(\"images\"))\n",
    "\n",
    "# Transformer that extracts and flattens the richly structured output of Bing Image Search into a simple URL column\n",
    "getUrls = BingImageSearch.getUrlTransformer(\"images\", \"url\")\n",
    "\n",
    "# This displays the full results returned, uncomment to use\n",
    "# display(bingSearch.transform(bingParameters))\n",
    "\n",
    "# Since we have two services, they are put into a pipeline\n",
    "pipeline = PipelineModel(stages=[bingSearch, getUrls])\n",
    "\n",
    "# Show the results of your search: image URLs\n",
    "display(pipeline.transform(bingParameters))"
   ]
  },
  {
   "source": [
    "## Speech-to-Text sample\n",
    "The [Speech-to-text](../speech-service/index-speech-to-text.yml) service converts streams or files of spoken audio to text. In this sample, we transcribe one audio file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with our audio URLs, tied to the column called \"url\"\n",
    "df = spark.createDataFrame([(\"https://mmlspark.blob.core.windows.net/datasets/Speech/audio2.wav\",)\n",
    "                           ], [\"url\"])\n",
    "\n",
    "# Run the Speech-to-text service to translate the audio into text\n",
    "speech_to_text = (SpeechToTextSDK()\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setLocation(\"eastus\")\n",
    "    .setOutputCol(\"text\")\n",
    "    .setAudioDataCol(\"url\")\n",
    "    .setLanguage(\"en-US\")\n",
    "    .setProfanity(\"Masked\"))\n",
    "\n",
    "# Show the results of the translation\n",
    "display(speech_to_text.transform(df).select(\"url\", \"text.DisplayText\"))"
   ]
  },
  {
   "source": [
    "## Anomaly Detector sample\n",
    "\n",
    "[Anomaly Detector](../anomaly-detector/index.yml) is great for detecting irregularities in your time series data. In this sample, we use the service to find anomalies in the entire time series."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create a dataframe with the point data that Anomaly Detector requires\n",
    "df = spark.createDataFrame([\n",
    "    (\"1972-01-01T00:00:00Z\", 826.0),\n",
    "    (\"1972-02-01T00:00:00Z\", 799.0),\n",
    "    (\"1972-03-01T00:00:00Z\", 890.0),\n",
    "    (\"1972-04-01T00:00:00Z\", 900.0),\n",
    "    (\"1972-05-01T00:00:00Z\", 766.0),\n",
    "    (\"1972-06-01T00:00:00Z\", 805.0),\n",
    "    (\"1972-07-01T00:00:00Z\", 821.0),\n",
    "    (\"1972-08-01T00:00:00Z\", 20000.0),\n",
    "    (\"1972-09-01T00:00:00Z\", 883.0),\n",
    "    (\"1972-10-01T00:00:00Z\", 898.0),\n",
    "    (\"1972-11-01T00:00:00Z\", 957.0),\n",
    "    (\"1972-12-01T00:00:00Z\", 924.0),\n",
    "    (\"1973-01-01T00:00:00Z\", 881.0),\n",
    "    (\"1973-02-01T00:00:00Z\", 837.0),\n",
    "    (\"1973-03-01T00:00:00Z\", 9000.0)\n",
    "], [\"timestamp\", \"value\"]).withColumn(\"group\", lit(\"series1\"))\n",
    "\n",
    "# Run the Anomaly Detector service to look for irregular data\n",
    "anamoly_detector = (SimpleDetectAnomalies()\n",
    "  .setSubscriptionKey(anomaly_key)\n",
    "  .setLocation(\"eastus\")\n",
    "  .setTimestampCol(\"timestamp\")\n",
    "  .setValueCol(\"value\")\n",
    "  .setOutputCol(\"anomalies\")\n",
    "  .setGroupbyCol(\"group\")\n",
    "  .setGranularity(\"monthly\"))\n",
    "\n",
    "# Show the full results of the analysis with the anomalies marked as \"True\"\n",
    "display(anamoly_detector.transform(df).select(\"timestamp\", \"value\", \"anomalies.isAnomaly\"))"
   ]
  },
  {
   "source": [
    "## Arbitrary web APIs\n",
    "\n",
    "With HTTP on Spark, any web service can be used in your big data pipeline. In this example, we use the [World Bank API](http://api.worldbank.org/v2/country/) to get information about various countries around the world."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Request\n",
    "from mmlspark.io.http import HTTPTransformer, http_udf\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# Use any requests from the python requests library\n",
    "def world_bank_request(country):\n",
    "  return Request(\"GET\", \"http://api.worldbank.org/v2/country/{}?format=json\".format(country))\n",
    "\n",
    "# Create a dataframe with spcificies which countries we want data on\n",
    "df = (spark.createDataFrame([(\"br\",),(\"usa\",)], [\"country\"])\n",
    "  .withColumn(\"request\", http_udf(world_bank_request)(col(\"country\"))))\n",
    "\n",
    "# Much faster for big data because of the concurrency :)\n",
    "client = (HTTPTransformer()\n",
    "      .setConcurrency(3)\n",
    "      .setInputCol(\"request\")\n",
    "      .setOutputCol(\"response\"))\n",
    "\n",
    "# Get the body of the response\n",
    "def get_response_body(resp):\n",
    "  return resp.entity.content.decode()\n",
    "\n",
    "# Show the details of the country data returned\n",
    "display(client.transform(df).select(\"country\", udf(get_response_body)(col(\"response\")).alias(\"response\")))"
   ]
  },
  {
   "source": [
    "## Azure Cognitive search - Creating a searchable Art Database with The MET's open-access collection sample\n",
    "\n",
    "In this example, we show how you can enrich data using Cognitive Skills and write to an Azure Search Index using MMLSpark. We use a subset of The MET's open-access collection and enrich it by passing it through 'Describe Image' and a custom 'Image Similarity' skill. The results are then written to a searchable index."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json, requests\n",
    "from pyspark.ml import Transformer, Estimator, Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.sql.functions import lit, udf, col, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISION_API_KEY = os.environ['VISION_API_KEY']\n",
    "AZURE_SEARCH_KEY = os.environ['AZURE_SEARCH_KEY']\n",
    "search_service = \"mmlspark-azure-search\"\n",
    "search_index = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read\\\n",
    "  .format(\"csv\")\\\n",
    "  .option(\"header\", True)\\\n",
    "  .load(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/metartworks_sample.csv\")\\\n",
    "  .withColumn(\"searchAction\", lit(\"upload\"))\\\n",
    "  .withColumn(\"Neighbors\", split(col(\"Neighbors\"), \",\").cast(\"array<string>\"))\\\n",
    "  .withColumn(\"Tags\", split(col(\"Tags\"), \",\").cast(\"array<string>\"))\\\n",
    "  .limit(25)"
   ]
  },
  {
   "source": [
    "<img src=\"https://mmlspark.blob.core.windows.net/graphics/CognitiveSearchHyperscale/MetArtworkSamples.png\" width=\"800\" style=\"float: center;\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark.cognitive import AnalyzeImage\n",
    "from mmlspark.stages import SelectColumns\n",
    "\n",
    "#define pipeline\n",
    "describeImage = (AnalyzeImage()\n",
    "  .setSubscriptionKey(VISION_API_KEY)\n",
    "  .setLocation(\"eastus\")\n",
    "  .setImageUrlCol(\"PrimaryImageUrl\")\n",
    "  .setOutputCol(\"RawImageDescription\")\n",
    "  .setErrorCol(\"Errors\")\n",
    "  .setVisualFeatures([\"Categories\", \"Tags\", \"Description\", \"Faces\", \"ImageType\", \"Color\", \"Adult\"])\n",
    "  .setConcurrency(5))\n",
    "\n",
    "df2 = describeImage.transform(data)\\\n",
    "  .select(\"*\", \"RawImageDescription.*\").drop(\"Errors\", \"RawImageDescription\")"
   ]
  },
  {
   "source": [
    "<img src=\"https://mmlspark.blob.core.windows.net/graphics/CognitiveSearchHyperscale/MetArtworksProcessed.png\" width=\"800\" style=\"float: center;\"/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Before writing the results to a Search Index, you must define a schema which must specify the name, type, and attributes of each field in your index. Refer [Create a basic index in Azure Search](https://docs.microsoft.com/en-us/azure/search/search-what-is-an-index) for more information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark.cognitive import *\n",
    "df2.writeToAzureSearch(\n",
    "  subscriptionKey=AZURE_SEARCH_KEY,\n",
    "  actionCol=\"searchAction\",\n",
    "  serviceName=search_service,\n",
    "  indexName=search_index,\n",
    "  keyCol=\"ObjectID\"\n",
    ")"
   ]
  },
  {
   "source": [
    "The Search Index can be queried using the [Azure Search REST API](https://docs.microsoft.com/rest/api/searchservice/) by sending GET or POST requests and specifying query parameters that give the criteria for selecting matching documents. For more information on querying refer [Query your Azure Search index using the REST API](https://docs.microsoft.com/en-us/rest/api/searchservice/Search-Documents)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://{}.search.windows.net/indexes/{}/docs/search?api-version=2019-05-06'.format(search_service, search_index)\n",
    "requests.post(url, json={\"search\": \"Glass\"}, headers = {\"api-key\": AZURE_SEARCH_KEY}).json()"
   ]
  },
  {
   "source": [
    "## See also\n",
    "\n",
    "* [Recipe: Anomaly Detection](./recipes/anomaly-detection.md)\n",
    "* [Recipe: Art Explorer](./recipes/art-explorer.md)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}