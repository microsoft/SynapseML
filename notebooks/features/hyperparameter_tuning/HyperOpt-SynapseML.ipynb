{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning: SynapseML with Hyperopt\n",
    "\n",
    "[SynapseML](https://github.com/microsoft/SynapseML) is an open-source library that simplifies the creation of massively scalable machine learning (ML) pipelines. SynapseML provides simple, composable, and distributed APIs for a wide variety of different machine learning tasks such as text analytics, vision, anomaly detection, and many others.\n",
    "\n",
    "[Hyperopt](https://github.com/hyperopt/hyperopt), on the other hand, is a Python library for serial and parallel optimization over complex search spaces, including real-valued, discrete, and conditional dimensions.\n",
    "\n",
    "This guide showcases the process of tuning a distributed algorithm in Spark with SynapseML and Hyperopt.\n",
    "\n",
    "The use case of this guide is for distributed machine learning in Python that requires hyperparameter tuning. It provides a demo on how to tune hyperparameters for a machine learning workflow in SynapseML and can be used as a reference to tune other distributed machine learning algorithms from Spark MLlib or other libraries.\n",
    "\n",
    "The guide includes two sections:\n",
    "* Running distributed training with SynapseML without hyperparameter tuning.\n",
    "* Using Hyperopt to tune hyperparameters in the distributed training workflow.\n",
    "## Prerequisites\n",
    " - If you are running it on Synapse, you'll need to [create an AML workspace and set up linked Service](https://microsoft.github.io/SynapseML/docs/next/mlflow/installation/). \n",
    "\n",
    "## Requirements\n",
    " - Install HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install hyperopt\n",
    "import os\n",
    "\n",
    "os.system(\"pip install hyperopt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow autologging\n",
    "\n",
    "To track model training and tuning with MLflow, you could enable MLflow autologging by running `mlflow.pyspark.ml.autolog()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version >= 1.28.0 supports reading logModelAllowlistFile from url\n",
    "# %pip install mlflow==1.29.0\n",
    "os.system(\"pip install mlflow==1.29.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.core.platform import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "if running_on_synapse_internal():\n",
    "    experiment_name = \"hyperopt-synapseml\"\n",
    "elif running_on_synapse():\n",
    "    experiment_name = \"hyperopt-synapseml\"\n",
    "    # from notebookutils.visualization import display # use this display on interactive notebook\n",
    "    from synapse.ml.core.platform import (\n",
    "        materializing_display as display,\n",
    "    )  # display for pipeline testing\n",
    "else:\n",
    "    experiment_name = \"/Shared/hyperopt-synapseml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pyspark autologging logModelAllowlist to include SynapseML models\n",
    "spark.sparkContext._conf.set(\n",
    "    \"spark.mlflow.pysparkml.autolog.logModelAllowlistFile\",\n",
    "    \"https://mmlspark.blob.core.windows.net/publicwasb/log_model_allowlist.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable autologging\n",
    "mlflow.pyspark.ml.autolog()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set experiment name for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow experiment.\n",
    "\n",
    "if running_on_synapse():\n",
    "    from notebookutils.mssparkutils import azureML\n",
    "\n",
    "    linked_service = \"AzureMLService1\"  # use your linked service name\n",
    "    ws = azureML.getWorkspace(linked_service)\n",
    "    mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Run distributed training using MLlib\n",
    "\n",
    "This section shows a simple example of distributed training using SynapseML. For more information and examples, visit the official [website](https://microsoft.github.io/SynapseML/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "We use [*California Housing* dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset). \n",
    "The data was derived from the 1990 U.S. census. It consists of 20640 entries with 8 features. \n",
    "We use `sklearn.datasets` module to download it easily, then split the set into training and testing by 75/25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = fetch_california_housing()\n",
    "\n",
    "feature_cols = [\"f\" + str(i) for i in range(california.data.shape[1])]\n",
    "header = [\"target\"] + feature_cols\n",
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=np.column_stack((california.target, california.data)), columns=header\n",
    "    )\n",
    ").repartition(1)\n",
    "\n",
    "print(\"Dataframe has {} rows\".format(df.count()))\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the summary of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.summary().toPandas())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to train a model\n",
    "\n",
    "In this section, you define a function to train a gradient boosting model with SynapseML LightgbmRegressor.  Wrapping the training code in a function is important for passing the function to Hyperopt for tuning later.\n",
    "\n",
    "We evaluate the prediction result by using `synapse.ml.train.ComputeModelStatistics` which returns four metrics:\n",
    "* [MSE (Mean Squared Error)](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "* [RMSE (Root Mean Squared Error)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) = sqrt(MSE)\n",
    "* [R Squared](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "* [MAE (Mean Absolute Error)](https://en.wikipedia.org/wiki/Mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Convert features into a single vector column\n",
    "featurizer = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = featurizer.transform(df)[\"target\", \"features\"]\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.75, 0.25], seed=42)\n",
    "train_data, validation_data = train_data.randomSplit([0.85, 0.15], seed=42)\n",
    "\n",
    "display(train_data)\n",
    "\n",
    "# Using one partition since the training dataset is very small\n",
    "repartitioned_data = train_data.repartition(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.lightgbm import LightGBMRegressor\n",
    "from synapse.ml.train import ComputeModelStatistics\n",
    "\n",
    "\n",
    "def train_tree(alpha, learningRate, numLeaves, numIterations):\n",
    "    \"\"\"\n",
    "    This train() function:\n",
    "     - takes hyperparameters as inputs (for tuning later)\n",
    "     - returns the F1 score on the validation dataset\n",
    "\n",
    "    Wrapping code as a function makes it easier to reuse the code later with Hyperopt.\n",
    "    \"\"\"\n",
    "    # Use MLflow to track training.\n",
    "    # Specify \"nested=True\" since this single model will be logged as a child run of Hyperopt's run.\n",
    "    with mlflow.start_run(nested=True):\n",
    "\n",
    "        lgr = LightGBMRegressor(\n",
    "            objective=\"quantile\",\n",
    "            alpha=alpha,\n",
    "            learningRate=learningRate,\n",
    "            numLeaves=numLeaves,\n",
    "            labelCol=\"target\",\n",
    "            numIterations=numIterations,\n",
    "        )\n",
    "\n",
    "        model = lgr.fit(repartitioned_data)\n",
    "\n",
    "        cms = ComputeModelStatistics(\n",
    "            evaluationMetric=\"regression\", labelCol=\"target\", scoresCol=\"prediction\"\n",
    "        )\n",
    "\n",
    "        # Define an evaluation metric and evaluate the model on the test dataset.\n",
    "        predictions = model.transform(test_data)\n",
    "        metrics = cms.transform(predictions).collect()[0].asDict()\n",
    "\n",
    "        # log metrics with mlflow\n",
    "        mlflow.log_metric(\"MSE\", metrics[\"mean_squared_error\"])\n",
    "        mlflow.log_metric(\"RMSE\", metrics[\"root_mean_squared_error\"])\n",
    "        mlflow.log_metric(\"R^2\", metrics[\"R^2\"])\n",
    "        mlflow.log_metric(\"MAE\", metrics[\"mean_absolute_error\"])\n",
    "\n",
    "    return model, metrics[\"R^2\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training function to make sure it works.\n",
    "It's a good idea to make sure training code runs before adding in tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model, val_metric = train_tree(\n",
    "    alpha=0.2, learningRate=0.3, numLeaves=31, numIterations=100\n",
    ")\n",
    "print(\n",
    "    f\"The trained decision tree achieved a R^2 of {val_metric} on the validation data\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Use Hyperopt to tune hyperparameters\n",
    "\n",
    "In the second section, the Hyperopt workflow is created by:\n",
    "* Define a function to minimize\n",
    "* Define a search space over hyperparameters\n",
    "* Specifying the search algorithm and using `fmin()` for tuning the model.\n",
    "\n",
    "For more information about the Hyperopt APIs, see the [Hyperopt documentation](http://hyperopt.github.io/hyperopt/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to minimize\n",
    "\n",
    "* Input: hyperparameters\n",
    "* Internally: Reuse the training function defined above.\n",
    "* Output: loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "\n",
    "def train_with_hyperopt(params):\n",
    "    \"\"\"\n",
    "    An example train method that calls into MLlib.\n",
    "    This method is passed to hyperopt.fmin().\n",
    "\n",
    "    :param params: hyperparameters as a dict. Its structure is consistent with how search space is defined. See below.\n",
    "    :return: dict with fields 'loss' (scalar loss) and 'status' (success/failure status of run)\n",
    "    \"\"\"\n",
    "    # For integer parameters, make sure to convert them to int type if Hyperopt is searching over a continuous range of values.\n",
    "    alpha = params[\"alpha\"]\n",
    "    learningRate = params[\"learningRate\"]\n",
    "    numLeaves = int(params[\"numLeaves\"])\n",
    "    numIterations = int(params[\"numIterations\"])\n",
    "\n",
    "    model, r_squared = train_tree(alpha, learningRate, numLeaves, numIterations)\n",
    "\n",
    "    # Hyperopt expects you to return a loss (for which lower is better), so take the negative of the R^2 (for which higher is better).\n",
    "    loss = -r_squared\n",
    "\n",
    "    return {\"loss\": loss, \"status\": STATUS_OK}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the search space over hyperparameters\n",
    "\n",
    "This example tunes four hyperparameters: `alpha`, `learningRate`, `numLeaves` and `numIterations`. See the [Hyperopt documentation](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions) for details on defining a search space and parameter expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"alpha\": hp.uniform(\"alpha\", 0, 1),\n",
    "    \"learningRate\": hp.uniform(\"learningRate\", 0, 1),\n",
    "    \"numLeaves\": hp.uniformint(\"numLeaves\", 30, 50),\n",
    "    \"numIterations\": hp.uniformint(\"numIterations\", 100, 300),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the model using Hyperopt `fmin()`\n",
    "\n",
    "For tuning the model with Hyperopt's `fmin()`, the following steps are taken:\n",
    "- Setting `max_evals` to the maximum number of points in the hyperparameter space to be tested.\n",
    "- Specifying the search algorithm, either `hyperopt.tpe.suggest` or `hyperopt.rand.suggest`.\n",
    "  - `hyperopt.tpe.suggest`: Tree of Parzen Estimators, a Bayesian approach which iteratively and adaptively selects new hyperparameter settings to explore based on previous results\n",
    "  - `hyperopt.rand.suggest`: Random search, a non-adaptive approach that randomly samples the search space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:**  \n",
    "When using Hyperopt with SynapseML and other distributed training algorithms, do not pass a `trials` argument to `fmin()`. When you do not include the `trials` argument, Hyperopt uses the default `Trials` class, which runs on the cluster driver. Hyperopt needs to evaluate each trial on the driver node so that each trial can initiate distributed training jobs.  \n",
    "\n",
    "Do not use the `SparkTrials` class with SynapseML. `SparkTrials` is designed to distribute trials for algorithms that are not themselves distributed. SynapseML uses distributed computing already and is not compatible with `SparkTrials`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = tpe.suggest\n",
    "\n",
    "with mlflow.start_run():\n",
    "    best_params = fmin(fn=train_with_hyperopt, space=space, algo=algo, max_evals=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the parameters that produced the best model\n",
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model on the full training dataset\n",
    "\n",
    "For tuning, this workflow split the training dataset into training and validation subsets. Now, retrain the model using the \"best\" hyperparameters on the full training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = best_params[\"alpha\"]\n",
    "best_learningRate = best_params[\"learningRate\"]\n",
    "best_numIterations = int(best_params[\"numIterations\"])\n",
    "best_numLeaves = int(best_params[\"numLeaves\"])\n",
    "\n",
    "final_model, val_r_squared = train_tree(\n",
    "    best_alpha, best_learningRate, best_numIterations, best_numLeaves\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the test dataset to compare evaluation metrics for the initial and \"best\" models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an evaluation metric and evaluate the model on the test dataset.\n",
    "cms = ComputeModelStatistics(\n",
    "    evaluationMetric=\"regression\", labelCol=\"target\", scoresCol=\"prediction\"\n",
    ")\n",
    "\n",
    "initial_model_predictions = initial_model.transform(test_data)\n",
    "initial_model_test_metric = (\n",
    "    cms.transform(initial_model_predictions).collect()[0].asDict()[\"R^2\"]\n",
    ")\n",
    "\n",
    "final_model_predictions = final_model.transform(test_data)\n",
    "final_model_test_metric = (\n",
    "    cms.transform(final_model_predictions).collect()[0].asDict()[\"R^2\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"On the test data, the initial (untuned) model achieved R^2 {initial_model_test_metric}, and the final (tuned) model achieved {final_model_test_metric}.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}