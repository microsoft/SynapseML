{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating, Training and Evaluating Uplift Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll demonstrate how to create, train and evaluate uplift models and apply uplift modelling technique.\n",
    "\n",
    "- What is uplift modelling?\n",
    "\n",
    "    It is a family of causal inference technology that uses machine learning models to estimate the causal impact of some treatment on an individual's behaviour.\n",
    "\n",
    "    - **Persuadables** will only respond positive to the treatment\n",
    "    - **Sleeping-dogs** have a strong negative response to the treatment\n",
    "    - **Lost Causes** will never reach the outcome even with the treatment\n",
    "    - **Sure Things** will always reach the outcome with or without the treatment\n",
    "\n",
    "    The goal of uplift modelling is to identify the \"persuadables\", not waste efforts on \"sure things\" and \"lost causes\", and avoid bothering \"sleeping dogs\"\n",
    "\n",
    "- How does uplift modelling work?\n",
    "    - **Meta Learner**: predicts the difference between an individual's behaviour when there is a treatment and when there is no treatment\n",
    "\n",
    "    - **Uplift Tree**: a tree-based algorithm where the splitting criterion is based on differences in uplift\n",
    "\n",
    "    - **NN-based Model**ï¼ša neural network model that usually works with observational data\n",
    "\n",
    "- Where can uplift modelling work?\n",
    "    - Marketing: help to identify persuadables to apply a treatment such as a coupon or an online advertisement\n",
    "    - Medical Treatment: help to understand how a treatment can impact certain groups differently\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Notebook Configurations\n",
    "\n",
    "By defining below parameters, we can apply this notebook on different datasets easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "IS_CUSTOMER_DATA = False  # if True, dataset has to be uploaded manually by user\n",
    "DATA_FOLDER = \"Files/uplift-modelling\"\n",
    "DATA_FILE = \"criteo-research-uplift-v2.1.csv\"\n",
    "\n",
    "# data schema\n",
    "FEATURE_COLUMNS = [f\"f{i}\" for i in range(12)]\n",
    "TREATMENT_COLUMN = \"treatment\"\n",
    "LABEL_COLUMN = \"visit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from synapse.ml.featurize import Featurize\n",
    "from synapse.ml.lightgbm import *\n",
    "from synapse.ml.train import ComputeModelStatistics\n",
    "\n",
    "import os\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Download dataset and upload to lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "- Dataset description: This dataset was created by The Criteo AI Lab.The dataset consists of 13M rows, each one representing a user with 12 features, a treatment indicator and 2 binary labels (visits and conversions).\n",
    "    - f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11: feature values (dense, float)\n",
    "    - treatment: treatment group (1 = treated, 0 = control) which indicates if a customer was targeted by advertising randomly\n",
    "    - conversion: whether a conversion occured for this user (binary, label)\n",
    "    - visit: whether a visit occured for this user (binary, label)\n",
    "\n",
    "- Dataset homepage: https://ailab.criteo.com/criteo-uplift-prediction-dataset/\n",
    "\n",
    "- Citation:\n",
    "    ```\n",
    "    @inproceedings{Diemert2018,\n",
    "    author = {{Diemert Eustache, Betlei Artem} and Renaudin, Christophe and Massih-Reza, Amini},\n",
    "    title={A Large Scale Benchmark for Uplift Modeling},\n",
    "    publisher = {ACM},\n",
    "    booktitle = {Proceedings of the AdKDD and TargetAd Workshop, KDD, London,United Kingdom, August, 20, 2018},\n",
    "    year = {2018}\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "if not IS_CUSTOMER_DATA:\n",
    "    # Download demo data files into lakehouse if not exist\n",
    "    remote_url = \"http://go.criteo.net/criteo-research-uplift-v2.1.csv.gz\"\n",
    "    download_file = \"criteo-research-uplift-v2.1.csv.gz\"\n",
    "\n",
    "    # For this demo, we first check if the dataset files are already prepared in the default lakehouse. If not, we'll download the dataset.\n",
    "    import os\n",
    "    import requests\n",
    "\n",
    "    if not os.path.exists(\"/lakehouse/default\"):\n",
    "        # ask user to add a lakehouse if no default lakehouse added to the notebook.\n",
    "        # a new notebook will not link to any lakehouse by default.\n",
    "        raise FileNotFoundError(\n",
    "            \"Default lakehouse not found, please add a lakehouse for the notebook.\"\n",
    "        )\n",
    "    else:\n",
    "        # check if the needed files are already in the lakehouse, try to download if not.\n",
    "        # raise an error if downloading failed.\n",
    "        os.makedirs(f\"/lakehouse/default/{DATA_FOLDER}/raw/\", exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(f\"/lakehouse/default/{DATA_FOLDER}/raw/{DATA_FILE}\"):\n",
    "            try:\n",
    "                r = requests.get(f\"{remote_url}\", timeout=30)\n",
    "                with open(\n",
    "                    f\"/lakehouse/default/{DATA_FOLDER}/raw/{download_file}\", \"wb\"\n",
    "                ) as f:\n",
    "                    f.write(r.content)\n",
    "                print(f\"Downloaded {download_file} into {DATA_FOLDER}/raw/.\")\n",
    "\n",
    "                with gzip.open(\n",
    "                    f\"/lakehouse/default/{DATA_FOLDER}/raw/{download_file}\", \"rb\"\n",
    "                ) as fin:\n",
    "                    with open(\n",
    "                        f\"/lakehouse/default/{DATA_FOLDER}/raw/{DATA_FILE}\", \"wb\"\n",
    "                    ) as fout:\n",
    "                        fout.write(fin.read())\n",
    "                print(f\"Unzip {download_file} into {DATA_FOLDER}/raw/{DATA_FILE}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed on downloading {DATA_FILE}, error message: {e}\")\n",
    "        else:\n",
    "            print(f\"{DATA_FILE} already exists in {DATA_FOLDER}/raw/.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Read data from lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read.csv(f\"{DATA_FOLDER}/raw/{DATA_FILE}\", header=True, inferSchema=True)\n",
    "display(raw_df.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Data exploration\n",
    "\n",
    "- **The overall rate of users that visit/convert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"cbdb21d2-1420-495e-a169-c60f8314af7f\",\"activityId\":\"3806e4c7-a95e-44fb-b0e8-de2a0203b554\",\"applicationId\":\"application_1661918651252_0001\",\"jobGroupId\":\"9\",\"advices\":{\"warn\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "raw_df.select(\n",
    "    F.mean(\"visit\").alias(\"Percentage of users that visit\"),\n",
    "    F.mean(\"conversion\").alias(\"Percentage of users that convert\"),\n",
    "    (F.sum(\"conversion\") / F.sum(\"visit\")).alias(\"Percentage of visitors that convert\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "- **The overall average treatment effect on visit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"cbdb21d2-1420-495e-a169-c60f8314af7f\",\"activityId\":\"3806e4c7-a95e-44fb-b0e8-de2a0203b554\",\"applicationId\":\"application_1661918651252_0001\",\"jobGroupId\":\"10\",\"advices\":{\"warn\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "raw_df.groupby(\"treatment\").agg(\n",
    "    F.mean(\"visit\").alias(\"Mean of visit\"),\n",
    "    F.sum(\"visit\").alias(\"Sum of visit\"),\n",
    "    F.count(\"visit\").alias(\"Count\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "- **The overall average treatment effect on conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"cbdb21d2-1420-495e-a169-c60f8314af7f\",\"activityId\":\"3806e4c7-a95e-44fb-b0e8-de2a0203b554\",\"applicationId\":\"application_1661918651252_0001\",\"jobGroupId\":\"11\",\"advices\":{\"warn\":1}}"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "raw_df.groupby(\"treatment\").agg(\n",
    "    F.mean(\"conversion\").alias(\"Mean of conversion\"),\n",
    "    F.sum(\"conversion\").alias(\"Sum of conversion\"),\n",
    "    F.count(\"conversion\").alias(\"Count\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Split train-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "transformer = (\n",
    "    Featurize().setOutputCol(\"features\").setInputCols(FEATURE_COLUMNS).fit(raw_df)\n",
    ")\n",
    "\n",
    "df = transformer.transform(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Size of train dataset: %d\" % train_df.count())\n",
    "print(\"Size of test dataset: %d\" % test_df.count())\n",
    "\n",
    "train_df.groupby(TREATMENT_COLUMN).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Split treatment-control dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "treatment_train_df = train_df.where(f\"{TREATMENT_COLUMN} > 0\")\n",
    "control_train_df = train_df.where(f\"{TREATMENT_COLUMN} = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Uplift Modelling: T-Learner with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def train(train_df):\n",
    "    classifier = (\n",
    "        LightGBMClassifier()\n",
    "        .setFeaturesCol(\"features\")\n",
    "        .setNumLeaves(10)\n",
    "        .setNumIterations(100)\n",
    "        .setObjective(\"binary\")\n",
    "        .setLabelCol(LABEL_COLUMN)\n",
    "    )\n",
    "\n",
    "    model = classifier.fit(train_df)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "treatment_model = train(treatment_train_df)\n",
    "control_model = train(control_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "getPred = F.udf(lambda v: float(v[1]), FloatType())\n",
    "\n",
    "test_pred_df = (\n",
    "    treatment_model.transform(test_df)\n",
    "    .withColumn(\"treatment_pred\", getPred(\"probability\"))\n",
    "    .drop(\"rawPrediction\", \"probability\", \"prediction\")\n",
    ")\n",
    "\n",
    "test_pred_df = (\n",
    "    control_model.transform(test_pred_df)\n",
    "    .withColumn(\"control_pred\", getPred(\"probability\"))\n",
    "    .drop(\"rawPrediction\", \"probability\", \"prediction\")\n",
    ")\n",
    "\n",
    "test_pred_df = test_pred_df.withColumn(\n",
    "    \"lift_pred\", F.col(\"treatment_pred\") - F.col(\"control_pred\")\n",
    ").select(TREATMENT_COLUMN, LABEL_COLUMN, \"treatment_pred\", \"control_pred\", \"lift_pred\")\n",
    "\n",
    "display(test_pred_df.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Model evaluation\n",
    "\n",
    "Since actial uplift cannot be observed for each individual, we measure the uplift over a group of customers.\n",
    "\n",
    "- **Uplift Curve**: plots the real cumulative uplift across the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "test_pred_pandas_df = test_pred_df.toPandas()\n",
    "test_pred_pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "First, we define some helper functions to plot uplift curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def uplift_rank(uplift_df, treatment_col, label_col, uplift_col):\n",
    "    # Rank the data by the uplift score\n",
    "    ranked = pd.DataFrame(\n",
    "        {\"treatment\": [], \"label\": [], \"uplift_score\": [], \"ranked_uplift\": []}\n",
    "    )\n",
    "    ranked[\"treatment\"] = uplift_df[treatment_col]\n",
    "    ranked[\"label\"] = uplift_df[label_col]\n",
    "    ranked[\"uplift_score\"] = uplift_df[uplift_col]\n",
    "    ranked[\"ranked_uplift\"] = ranked.uplift_score.rank(pct=True, ascending=False)\n",
    "    ranked = ranked.sort_values(by=\"ranked_uplift\").reset_index(drop=True)\n",
    "    return ranked\n",
    "\n",
    "\n",
    "def uplift_eval(ranked):\n",
    "    uplift_df = ranked.copy()\n",
    "    # Using Treatment and Control Group to calculate the uplift (Incremental gain)\n",
    "    C, T = sum(ranked.treatment == 0), sum(ranked.treatment != 0)\n",
    "    ranked[\"cr\"] = ranked.label\n",
    "    ranked[\"tr\"] = ranked.label\n",
    "    ranked.loc[ranked.treatment != 0, \"cr\"] = 0\n",
    "    ranked.loc[ranked.treatment == 0, \"tr\"] = 0\n",
    "    ranked[\"cr/c\"] = ranked.cr.cumsum() / C\n",
    "    ranked[\"tr/t\"] = ranked.tr.cumsum() / T\n",
    "    # Calculate and put the uplift value into dataframe\n",
    "    uplift_df[\"uplift\"] = round(ranked[\"tr/t\"] - ranked[\"cr/c\"], 5)\n",
    "\n",
    "    # Add q0\n",
    "    q0 = pd.DataFrame({\"ranked_uplift\": 0, \"uplift\": 0, \"treatment\": None}, index=[0])\n",
    "    uplift_df = pd.concat([q0, uplift_df]).reset_index(drop=True)\n",
    "\n",
    "    return uplift_df\n",
    "\n",
    "\n",
    "def uplift_plot(uplift_df):\n",
    "    gain_x = uplift_df.ranked_uplift\n",
    "    gain_y = uplift_df.uplift\n",
    "    # plot the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mpl.rcParams[\"font.size\"] = 8\n",
    "\n",
    "    ax = plt.plot(gain_x, gain_y, color=\"#2077B4\", label=\"Normalized Uplift Model\")\n",
    "\n",
    "    plt.plot(\n",
    "        [0, gain_x.max()],\n",
    "        [0, gain_y.max()],\n",
    "        \"--\",\n",
    "        color=\"tab:orange\",\n",
    "        label=\"Random Treatment\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Porportion Targeted\")\n",
    "    plt.ylabel(\"Uplift\")\n",
    "    plt.grid(b=True, which=\"major\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now we can plot the uplift curve on the prediction of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ranked_df = uplift_rank(\n",
    "    test_pred_pandas_df,\n",
    "    treatment_col=TREATMENT_COLUMN,\n",
    "    label_col=LABEL_COLUMN,\n",
    "    uplift_col=\"lift_pred\",\n",
    ")\n",
    "uplift_df = uplift_eval(ranked_df)\n",
    "uplift_plot(uplift_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "From the uplift curve above, we notice that the top 20% population ranked by our prediction have a large gain if they were given the treatment, which means the are the **persuadables**. Therefore, we can print the cutoff score at 20% percentage to identify the target customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "cutoff_percentage = 0.2\n",
    "cutoff_score = ranked_df.iloc[int(len(ranked_df) * cutoff_percentage)][\"uplift_score\"]\n",
    "\n",
    "print(\"Uplift score higher than {:.4f} are Persuadables\".format(cutoff_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "notebook_environment": {},
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.livy.synapse.ipythonInterpreter.enabled": "true"
    },
    "enableDebugMode": false,
    "keepAliveTimeout": 30
   }
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "4eb0cf6c-9e08-4640-bc5c-206cba1864b2",
    "known_lakehouses": [
     {
      "id": "4eb0cf6c-9e08-4640-bc5c-206cba1864b2"
     }
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "1d10ca6e668f54b54a282e8fffa4324e72130593ca9a1b635e16a1de3383a887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
