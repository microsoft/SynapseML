{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 301 - Ingesting CIFAR Images into Spark DataFrames and Evaluating Pre-Trained CNTK Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import CNTKModel, ModelDownloader\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from os.path import abspath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdnURL = \"https://mmlspark.azureedge.net/datasets\"\n",
    "\n",
    "# Please note that this is a copy of the CIFAR10 dataset originally found here:\n",
    "# http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "dataFile = \"cifar-10-python.tar.gz\"\n",
    "dataURL = cdnURL + \"/CIFAR10/\" + dataFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "mml-deploy": "hdinsight"
   },
   "outputs": [],
   "source": [
    "modelName = \"ConvNet\"\n",
    "modelDir = \"wasb:///models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model and extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tarfile, pickle\n",
    "import urllib.request\n",
    "\n",
    "d = ModelDownloader(spark, modelDir)\n",
    "model = d.downloadByName(modelName)\n",
    "if not os.path.isfile(dataFile):\n",
    "    urllib.request.urlretrieve(dataURL, dataFile)\n",
    "with tarfile.open(dataFile, \"r:gz\") as f:\n",
    "    test_dict = pickle.load(f.extractfile(\"cifar-10-batches-py/test_batch\"),\n",
    "                            encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def reshape_image(record):\n",
    "    image, label, filename = record\n",
    "    data = [float(x) for x in image.reshape(3,32,32).flatten()]\n",
    "    return data, label, filename\n",
    "\n",
    "convert_to_float = udf(lambda x: x, ArrayType(FloatType()))\n",
    "\n",
    "image_rdd = zip(test_dict[\"data\"], test_dict[\"labels\"], test_dict[\"filenames\"])\n",
    "image_rdd = spark.sparkContext.parallelize(image_rdd).map(reshape_image)\n",
    "\n",
    "imagesWithLabels = image_rdd.toDF([\"images\", \"labels\", \"filename\"])\n",
    "imagesWithLabels = imagesWithLabels.withColumn(\"images\", convert_to_float(col(\"images\")))\n",
    "imagesWithLabels.printSchema()\n",
    "\n",
    "imagesWithLabels.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate CNTK model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Use CNTK model to get log probabilities\n",
    "cntkModel = CNTKModel().setInputCol(\"images\").setOutputCol(\"output\") \\\n",
    "                       .setModelLocation(spark, model.uri).setOutputNodeName(\"z\")\n",
    "scoredImages = cntkModel.transform(imagesWithLabels)\n",
    "\n",
    "# Transform the log probabilities to predictions\n",
    "def argmax(x): return max(enumerate(x),key=lambda p: p[1])[0]\n",
    "\n",
    "argmaxUDF = udf(argmax, IntegerType())\n",
    "imagePredictions = scoredImages.withColumn(\"predictions\", argmaxUDF(\"output\")) \\\n",
    "                               .select(\"predictions\", \"labels\")\n",
    "\n",
    "numRows = imagePredictions.count()\n",
    "\n",
    "end = time.time()\n",
    "print(\"classifying {} images took {} seconds\".format(numRows,end-start))\n",
    "\n",
    "# Register the predictions as a temp table for further analysis using SQL\n",
    "imagePredictions.registerTempTable(\"ImagePredictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "mml-deploy": "hdinsight"
   },
   "outputs": [],
   "source": [
    "%%sql -q -o imagePredictions\n",
    "select * from ImagePredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "mml-deploy": "hdinsight"
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "y, y_hat = imagePredictions[\"labels\"], imagePredictions[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y, y_hat)\n",
    "\n",
    "labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
    "          \"horse\", \"ship\", \"truck\"]\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels, rotation=90)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
