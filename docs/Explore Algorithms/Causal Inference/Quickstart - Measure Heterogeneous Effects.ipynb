{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Startup Investment Attribution - Understand Outreach Effort's Effect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This sample notebook aims to show the application of using SynapseML's DoubleMLEstimator for inferring causality using observational data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "A startup that sells software would like to know whether its outreach efforts were successful in attracting new customers or boosting consumption among existing customers. In other words, they would like to learn the treatment effect of each investment on customers' software usage.\n",
    "\n",
    "In an ideal world, the startup would run several randomized experiments where each customer would receive a random assortment of investments. However, this can be logistically prohibitive or strategically unsound: the startup might not have the resources to design such experiments or they might not want to risk losing out on big opportunities due to lack of incentives.\n",
    "\n",
    "In this customer scenario walkthrough, we show how SynapseML causal package can use historical investment data to learn the investment effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Background\n",
    "In this scenario, a startup that sells software provides discounts incentives to its customer. A customer might be given or not.\n",
    "\n",
    "The startup has historical data on these investments for 2,000 customers, as well as how much revenue these customers generated in the year after the investments were made. They would like to use this data to learn the optimal incentive policy for each existing or new customer in order to maximize the return on investment (ROI).\n",
    "\n",
    "The startup faces a challenge:  the dataset is biased because historically the larger customers received the most incentives. Thus, they need a causal model that can remove the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Data\n",
    "The data* contains ~2,000 customers and is comprised of:\n",
    "\n",
    "* Customer features: details about the industry, size, revenue, and technology profile of each customer.\n",
    "* Interventions: information about which incentive was given to a customer.\n",
    "* Outcome: the amount of product the customer bought in the year after the incentives were given.\n",
    "\n",
    "\n",
    "| Feature Name    | Type | Details                                                                                                                                     |\n",
    "|-----------------|------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Global Flag     | W    | whether the customer has global offices                                                                                                     | \n",
    "| Major Flag      | W    | whether the customer is a large consumer in their industry (as opposed to SMC - Small Medium Corporation - or SMB - Small Medium Business)  |\n",
    "| SMC Flag        | W    | whether the customer is a Small Medium Corporation (SMC, as opposed to major and SMB)                                                       |\n",
    "| Commercial Flag | W    | whether the customer's business is commercial (as opposed to public secor)                                                                  |\n",
    "| IT Spend        | W    | dollar spent on IT-related purchases                                                                                                             |\n",
    "| Employee Count  | W    | number of employees                                                                                                                         |\n",
    "| PC Count        | W    | number of PCs used by the customer                                                                                                          |                                                                                      |\n",
    "| Size            | X    | customer's size given by their yearly total revenue                                                                                        |                                                                                      |\n",
    "| Discount        | T    | whether the customer was given a discount (binary)                                                                                          |\n",
    "| Revenue         | Y    | $ Revenue from customer given by the amount of software purchased                                                                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the sample multi-attribution data\n",
    "data = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\n",
    "        \"wasbs://publicwasb@mmlspark.blob.core.windows.net/multi_attribution_sample.csv\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Get Heterogenous Causal Effects with SynapseML OrthoDML Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.causal import *\n",
    "from pyspark.ml import Pipeline\n",
    "from synapse.ml.causal import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import IntegerType, BooleanType, DateType, DoubleType\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "treatmentColumn = \"Discount\"\n",
    "outcomeColumn = \"Revenue\"\n",
    "confounderColumns = [\n",
    "    \"Global Flag\",\n",
    "    \"Major Flag\",\n",
    "    \"SMC Flag\",\n",
    "    \"Commercial Flag\",\n",
    "    \"Employee Count\",\n",
    "    \"PC Count\",\n",
    "]\n",
    "heteroColumns = [\"Size\", \"IT Spend\"]\n",
    "heterogeneityVecCol = \"XVec\"\n",
    "confounderVecCol = \"XWVec\"\n",
    "\n",
    "data = data.withColumn(treatmentColumn, data.Discount.cast(DoubleType()))\n",
    "\n",
    "heterogeneityVector = VectorAssembler(\n",
    "    inputCols=heteroColumns, outputCol=heterogeneityVecCol\n",
    ")\n",
    "\n",
    "confounderVector = VectorAssembler(\n",
    "    inputCols=confounderColumns, outputCol=confounderVecCol\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[heterogeneityVector, confounderVector])\n",
    "\n",
    "ppfit = pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "### Create the Ortho Forest DML Estimator Model\n",
    "mtTransform = (\n",
    "    OrthoForestDMLEstimator()\n",
    "    .setNumTrees(100)\n",
    "    .setTreatmentCol(treatmentColumn)\n",
    "    .setOutcomeCol(outcomeColumn)\n",
    "    .setHeterogeneityVecCol(heterogeneityVecCol)\n",
    "    .setConfounderVecCol(confounderVecCol)\n",
    "    .setMaxDepth(10)\n",
    "    .setMinSamplesLeaf(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "### Fit the model for the data\n",
    "finalModel = mtTransform.fit(ppfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the input data to see the model in action\n",
    "finalPred = finalModel.transform(ppfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the data in Pandas\n",
    "pd_final = finalPred.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot and see the non-linear effects\n",
    "plt.scatter(\"Size\", mtTransform.getOutputCol(), data=pd_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
