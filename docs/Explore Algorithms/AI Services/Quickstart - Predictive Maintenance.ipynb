{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe: Predictive maintenance with the Azure AI Services for Big Data\n",
    "\n",
    "This recipe shows how you can use Azure Synapse Analytics and Azure AI services on Apache Spark for predictive maintenance of IoT devices. We'll follow along with the [CosmosDB and Synapse Link](https://github.com/Azure-Samples/cosmosdb-synapse-link-samples) sample. To keep things simple, in this recipe we'll read the data straight from a CSV file rather than getting streamed data through CosmosDB and Synapse Link. We strongly encourage you to look over the Synapse Link sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "Starting on the 20th of September, 2023 you wonâ€™t be able to create new Anomaly Detector resources. The Anomaly Detector service is being retired on the 1st of October, 2026."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical scenario\n",
    "\n",
    "The hypothetical scenario is a Power Plant, where IoT devices are monitoring [steam turbines](https://en.wikipedia.org/wiki/Steam_turbine). The IoTSignals collection has Revolutions per minute (RPM) and Megawatts (MW) data for each turbine. Signals from steam turbines are being analyzed and anomalous signals are detected.\n",
    "\n",
    "There could be outliers in the data in random frequency. In those situations, RPM values will go up and MW output will go down, for circuit protection. The idea is to see the data varying at the same time, but with different signals.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* An Azure subscription - [Create one for free](https://azure.microsoft.com/free/)\n",
    "* [Azure Synapse workspace](https://docs.microsoft.com/azure/synapse-analytics/get-started-create-workspace) configured with a [serverless Apache Spark pool](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-spark)\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Create an Anomaly Detector resource\n",
    "\n",
    "Azure AI Services are represented by Azure resources that you subscribe to. Create a resource for Translator using the [Azure portal](https://docs.microsoft.com/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Clinux) or [Azure CLI](https://learn.microsoft.com/azure/ai-services/multi-service-resource). You can also:\n",
    "\n",
    "- View an existing resource in the  [Azure portal](https://portal.azure.com/).\n",
    "\n",
    "Make note of the endpoint and the key for this resource, you'll need it in this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter your service keys\n",
    "\n",
    "Let's start by adding your key and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from synapse.ml.core.platform import find_secret\n",
    "\n",
    "service_key = find_secret(\"anomaly-api-key\")  # Paste your anomaly detector key here\n",
    "location = \"westus2\"  # Paste your anomaly detector location here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data into a DataFrame\n",
    "\n",
    "Next, let's read the IoTSignals file into a DataFrame. Open a new notebook in your Synapse workspace and create a DataFrame from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signals = spark.read.csv(\n",
    "    \"wasbs://publicwasb@mmlspark.blob.core.windows.net/iot/IoTSignals.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run anomaly detection using AI services on Spark\n",
    "\n",
    "The goal is to find instances where the signals from the IoT devices were outputting anomalous values so that we can see when something is going wrong and do predictive maintenance. To do that, let's use Anomaly Detector on Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct\n",
    "from synapse.ml.cognitive import SimpleDetectAnomalies\n",
    "from synapse.ml.core.spark import FluentAPI\n",
    "\n",
    "detector = (\n",
    "    SimpleDetectAnomalies()\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setLocation(location)\n",
    "    .setOutputCol(\"anomalies\")\n",
    "    .setGroupbyCol(\"grouping\")\n",
    "    .setSensitivity(95)\n",
    "    .setGranularity(\"secondly\")\n",
    ")\n",
    "\n",
    "df_anomaly = (\n",
    "    df_signals.where(col(\"unitSymbol\") == \"RPM\")\n",
    "    .withColumn(\"timestamp\", col(\"dateTime\").cast(\"string\"))\n",
    "    .withColumn(\"value\", col(\"measureValue\").cast(\"double\"))\n",
    "    .withColumn(\"grouping\", struct(\"deviceId\"))\n",
    "    .mlTransform(detector)\n",
    ").cache()\n",
    "\n",
    "df_anomaly.createOrReplaceTempView(\"df_anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly.select(\"timestamp\", \"value\", \"deviceId\", \"anomalies.isAnomaly\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell should yield a result that looks like:\n",
    "\n",
    "| timestamp           |   value | deviceId   | isAnomaly   |\n",
    "|:--------------------|--------:|:-----------|:------------|\n",
    "| 2020-05-01 18:33:51 |    3174 | dev-7      | False       |\n",
    "| 2020-05-01 18:33:52 |    2976 | dev-7      | False       |\n",
    "| 2020-05-01 18:33:53 |    2714 | dev-7      | False       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize anomalies for one of the devices\n",
    "\n",
    "IoTSignals.csv has signals from multiple IoT devices. We'll focus on a specific device and visualize anomalous outputs from the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly_single_device = spark.sql(\n",
    "    \"\"\"\n",
    "select\n",
    "  timestamp,\n",
    "  measureValue,\n",
    "  anomalies.expectedValue,\n",
    "  anomalies.expectedValue + anomalies.upperMargin as expectedUpperValue,\n",
    "  anomalies.expectedValue - anomalies.lowerMargin as expectedLowerValue,\n",
    "  case when anomalies.isAnomaly=true then 1 else 0 end as isAnomaly\n",
    "from\n",
    "  df_anomaly\n",
    "where deviceid = 'dev-1' and timestamp < '2020-04-29'\n",
    "order by timestamp\n",
    "limit 200\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a dataframe that represents the anomalies for a particular device, we can visualize these anomalies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "adf = df_anomaly_single_device.toPandas()\n",
    "adf_subset = df_anomaly_single_device.where(col(\"isAnomaly\") == 1).toPandas()\n",
    "\n",
    "plt.figure(figsize=(23, 8))\n",
    "plt.plot(\n",
    "    adf[\"timestamp\"],\n",
    "    adf[\"expectedUpperValue\"],\n",
    "    color=\"darkred\",\n",
    "    linestyle=\"solid\",\n",
    "    linewidth=0.25,\n",
    "    label=\"UpperMargin\",\n",
    ")\n",
    "plt.plot(\n",
    "    adf[\"timestamp\"],\n",
    "    adf[\"expectedValue\"],\n",
    "    color=\"darkgreen\",\n",
    "    linestyle=\"solid\",\n",
    "    linewidth=2,\n",
    "    label=\"Expected Value\",\n",
    ")\n",
    "plt.plot(\n",
    "    adf[\"timestamp\"],\n",
    "    adf[\"measureValue\"],\n",
    "    \"b\",\n",
    "    color=\"royalblue\",\n",
    "    linestyle=\"dotted\",\n",
    "    linewidth=2,\n",
    "    label=\"Actual\",\n",
    ")\n",
    "plt.plot(\n",
    "    adf[\"timestamp\"],\n",
    "    adf[\"expectedLowerValue\"],\n",
    "    color=\"black\",\n",
    "    linestyle=\"solid\",\n",
    "    linewidth=0.25,\n",
    "    label=\"Lower Margin\",\n",
    ")\n",
    "plt.plot(adf_subset[\"timestamp\"], adf_subset[\"measureValue\"], \"ro\", label=\"Anomaly\")\n",
    "plt.legend()\n",
    "plt.title(\"RPM Anomalies with Confidence Intervals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, your output will look like this:\n",
    "\n",
    "![Anomaly Detector Plot](https://github.com/MicrosoftDocs/azure-docs/raw/master/articles/cognitive-services/big-data/media/anomaly-output.png)\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Learn how to do predictive maintenance at scale with Azure AI services, Azure Synapse Analytics, and Azure CosmosDB. For more information, see the full sample on [GitHub](https://github.com/Azure-Samples/cosmosdb-synapse-link-samples)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
