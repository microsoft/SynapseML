{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI services"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-synapse-internal",
     "hide-azure"
    ]
   },
   "source": [
    "<image width=\"200\" alt-text=\"icon\" src=\"https://mmlspark.blob.core.windows.net/graphics/Readme/cog_services_on_spark_2.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure AI services help developers and organizations rapidly create intelligent, cutting-edge, market-ready, and responsible applications with out-of-the-box and pre-built and customizable APIs and models.\n",
    "\n",
    "SynapseML allows you to build powerful and highly scalable predictive and analytical models from various Spark data sources. Synapse Spark provide built-in SynapseML libraries including synapse.ml.services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "important",
     "alert"
    ]
   },
   "source": [
    "## Important\n",
    "Starting on the 20th of September, 2023 you wonâ€™t be able to create new Anomaly Detector resources. The Anomaly Detector service is being retired on the 1st of October, 2026."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-synapse-internal",
     "hide-azure"
    ]
   },
   "source": [
    "## Prerequisites on Azure Databricks\n",
    "\n",
    "1. Follow the steps in [Getting started](https://docs.microsoft.com/azure/services-services/big-data/getting-started) to set up your Azure Databricks and Azure AI services environment. This tutorial shows you how to install SynapseML and how to create your Spark cluster in Databricks.\n",
    "1. After you create a new notebook in Azure Databricks, copy the **Shared code** below and paste into a new cell in your notebook.\n",
    "1. Choose a service sample, below, and copy paste it into a second new cell in your notebook.\n",
    "1. Replace any of the service subscription key placeholders with your own key.\n",
    "1. Choose the run button (triangle icon) in the upper right corner of the cell, then select **Run Cell**.\n",
    "1. View results in a table below the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-synapse-internal"
    ]
   },
   "source": [
    "## Prerequisites on Azure Synapse Analytics\n",
    "\n",
    "The tutorial, [Pre-requisites for using Azure AI services in Azure Synapse](https://learn.microsoft.com/azure/synapse-analytics/machine-learning/tutorial-configure-cognitive-services-synapse), walks you through a couple steps you need to perform before using Azure AI services in Synapse Analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your system\n",
    "\n",
    "To begin, import required libraries and initialize your Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from synapse.ml.io.http import HTTPTransformer, http_udf\n",
    "from requests import Request\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Azure AI services libraries and replace the keys and locations in the following code snippet with your Azure AI services key and location."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform sentiment analysis on text\n",
    "\n",
    "The [AI Language](https://azure.microsoft.com/products/ai-services/ai-language/) service provides several algorithms for extracting intelligent insights from text. For example, we can find the sentiment of given input text. The service will return a score between 0.0 and 1.0 where low scores indicate negative sentiment and high score indicates positive sentiment.  This sample uses three simple sentences and returns the sentiment for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that's tied to it's column names\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"I am so happy today, its sunny!\", \"en-US\"),\n",
    "        (\"I am frustrated by this rush hour traffic\", \"en-US\"),\n",
    "        (\"The AI services on spark aint bad\", \"en-US\"),\n",
    "    ],\n",
    "    [\"text\", \"language\"],\n",
    ")\n",
    "\n",
    "# Run the Text Analytics service with options\n",
    "sentiment = (\n",
    "    AnalyzeText()\n",
    "    .setKind(\"SentimentAnalysis\")\n",
    "    .setTextCol(\"text\")\n",
    "    .setLocation(service_loc)\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setOutputCol(\"sentiment\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setLanguageCol(\"language\")\n",
    ")\n",
    "\n",
    "# Show the results of your text query in a table format\n",
    "display(\n",
    "    sentiment.transform(df).select(\n",
    "        \"text\", col(\"sentiment.documents.sentiment\").alias(\"sentiment\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform text analytics for health data\n",
    "\n",
    "The [Text Analytics for Health Service](https://docs.microsoft.com/azure/ai-services/language-service/text-analytics-for-health/overview?tabs=ner) extracts and labels relevant medical information from unstructured text such as doctor's notes, discharge summaries, clinical documents, and electronic health records.\n",
    "\n",
    "The following code sample analyzes and transforms text from doctors notes into structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"20mg of ibuprofen twice a day\",),\n",
    "        (\"1tsp of Tylenol every 4 hours\",),\n",
    "        (\"6-drops of Vitamin B-12 every evening\",),\n",
    "    ],\n",
    "    [\"text\"],\n",
    ")\n",
    "\n",
    "healthcare = (\n",
    "    AnalyzeHealthText()\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setLocation(service_loc)\n",
    "    .setLanguage(\"en\")\n",
    "    .setOutputCol(\"response\")\n",
    ")\n",
    "\n",
    "display(healthcare.transform(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate text into a different language\n",
    "[Translator](https://azure.microsoft.com/services/ai-services/translator/) is a cloud-based machine translation service and is part of the Azure AI services family of AI APIs used to build intelligent apps. Translator is easy to integrate in your applications, websites, tools, and solutions. It allows you to add multi-language user experiences in 90 languages and dialects and can be used to translate text without hosting your own algorithm.\n",
    "\n",
    "The following code sample does a simple text translation by providing the sentences you want to translate and target languages you want to translate them to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, flatten\n",
    "\n",
    "# Create a dataframe including sentences you want to translate\n",
    "df = spark.createDataFrame(\n",
    "    [([\"Hello, what is your name?\", \"Bye\"],)],\n",
    "    [\n",
    "        \"text\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run the Translator service with options\n",
    "translate = (\n",
    "    Translate()\n",
    "    .setSubscriptionKey(translator_key)\n",
    "    .setLocation(translator_loc)\n",
    "    .setTextCol(\"text\")\n",
    "    .setToLanguage([\"zh-Hans\"])\n",
    "    .setOutputCol(\"translation\")\n",
    ")\n",
    "\n",
    "# Show the results of the translation.\n",
    "display(\n",
    "    translate.transform(df)\n",
    "    .withColumn(\"translation\", flatten(col(\"translation.translations\")))\n",
    "    .withColumn(\"translation\", col(\"translation.text\"))\n",
    "    .select(\"translation\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information from a document into structured data\n",
    "[Azure AI Document Intelligence](https://azure.microsoft.com/products/ai-services/ai-document-intelligence/) is a part of Azure Applied AI Services that lets you build automated data processing software using machine learning technology. With Azure AI Document Intelligence, you can identify and extract text, key/value pairs, selection marks, tables, and structure from your documents. The service outputs structured data that includes the relationships in the original file, bounding boxes, confidence and more.\n",
    "\n",
    "The following code sample analyzes a business card image and extracts its information into structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# Create a dataframe containing the source files\n",
    "imageDf = spark.createDataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"https://mmlspark.blob.core.windows.net/datasets/FormRecognizer/business_card.jpg\",\n",
    "        )\n",
    "    ],\n",
    "    [\n",
    "        \"source\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run the Form Recognizer service\n",
    "analyzeBusinessCards = (\n",
    "    AnalyzeBusinessCards()\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setLocation(service_loc)\n",
    "    .setImageUrlCol(\"source\")\n",
    "    .setOutputCol(\"businessCards\")\n",
    ")\n",
    "\n",
    "# Show the results of recognition.\n",
    "display(\n",
    "    analyzeBusinessCards.transform(imageDf)\n",
    "    .withColumn(\n",
    "        \"documents\", explode(col(\"businessCards.analyzeResult.documentResults.fields\"))\n",
    "    )\n",
    "    .select(\"source\", \"documents\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision sample\n",
    "\n",
    "[Azure AI Vision](https://azure.microsoft.com/products/ai-services/ai-vision/) analyzes images to identify structure such as faces, objects, and natural-language descriptions.\n",
    "\n",
    "The following code sample analyzes images and labels them with *tags*. Tags are one-word descriptions of things in the image, such as recognizable objects, people, scenery, and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the image URLs\n",
    "base_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/\"\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (base_url + \"objects.jpg\",),\n",
    "        (base_url + \"dog.jpg\",),\n",
    "        (base_url + \"house.jpg\",),\n",
    "    ],\n",
    "    [\n",
    "        \"image\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run the Computer Vision service. Analyze Image extracts information from/about the images.\n",
    "analysis = (\n",
    "    AnalyzeImage()\n",
    "    .setLocation(service_loc)\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setVisualFeatures(\n",
    "        [\"Categories\", \"Color\", \"Description\", \"Faces\", \"Objects\", \"Tags\"]\n",
    "    )\n",
    "    .setOutputCol(\"analysis_results\")\n",
    "    .setImageUrlCol(\"image\")\n",
    "    .setErrorCol(\"error\")\n",
    ")\n",
    "\n",
    "# Show the results of what you wanted to pull out of the images.\n",
    "display(analysis.transform(df).select(\"image\", \"analysis_results.description.tags\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform speech to text\n",
    "The [Speech-to-text](https://azure.microsoft.com/products/ai-services/ai-speech/) service converts streams or files of spoken audio to text. The following code sample transcribes one audio file to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with our audio URLs, tied to the column called \"url\"\n",
    "df = spark.createDataFrame(\n",
    "    [(\"https://mmlspark.blob.core.windows.net/datasets/Speech/audio2.wav\",)], [\"url\"]\n",
    ")\n",
    "\n",
    "# Run the Speech-to-text service to translate the audio into text\n",
    "speech_to_text = (\n",
    "    SpeechToTextSDK()\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setLocation(service_loc)\n",
    "    .setOutputCol(\"text\")\n",
    "    .setAudioDataCol(\"url\")\n",
    "    .setLanguage(\"en-US\")\n",
    "    .setProfanity(\"Masked\")\n",
    ")\n",
    "\n",
    "# Show the results of the translation\n",
    "display(speech_to_text.transform(df).select(\"url\", \"text.DisplayText\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform text to speech\n",
    "[Text to speech](https://azure.microsoft.com/products/ai-services/text-to-speech/) is a service that allows you to build apps and services that speak naturally, choosing from more than 270 neural voices across 119 languages and variants.\n",
    "\n",
    "The following code sample transforms text into an audio file that contains the content of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synapse.ml.services.speech import TextToSpeech\n",
    "\n",
    "fs = \"\"\n",
    "if running_on_databricks():\n",
    "    fs = \"dbfs:\"\n",
    "elif running_on_synapse_internal():\n",
    "    fs = \"Files\"\n",
    "\n",
    "# Create a dataframe with text and an output file location\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"Reading out loud is fun! Check out aka.ms/spark for more information\",\n",
    "            fs + \"/output.mp3\",\n",
    "        )\n",
    "    ],\n",
    "    [\"text\", \"output_file\"],\n",
    ")\n",
    "\n",
    "tts = (\n",
    "    TextToSpeech()\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setTextCol(\"text\")\n",
    "    .setLocation(service_loc)\n",
    "    .setVoiceName(\"en-US-JennyNeural\")\n",
    "    .setOutputFileCol(\"output_file\")\n",
    ")\n",
    "\n",
    "# Check to make sure there were no errors during audio creation\n",
    "display(tts.transform(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect anomalies in time series data\n",
    "\n",
    "If you don't have an anomaly detection resource created before Sep 20th 2023, you won't be able to create one. You may want to skip this part.\n",
    "\n",
    "[Anomaly Detector](https://azure.microsoft.com/services/cognitive-services/anomaly-detector/) is great for detecting irregularities in your time series data. The following code sample uses the Anomaly Detector service to find anomalies in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the point data that Anomaly Detector requires\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"1972-01-01T00:00:00Z\", 826.0),\n",
    "        (\"1972-02-01T00:00:00Z\", 799.0),\n",
    "        (\"1972-03-01T00:00:00Z\", 890.0),\n",
    "        (\"1972-04-01T00:00:00Z\", 900.0),\n",
    "        (\"1972-05-01T00:00:00Z\", 766.0),\n",
    "        (\"1972-06-01T00:00:00Z\", 805.0),\n",
    "        (\"1972-07-01T00:00:00Z\", 821.0),\n",
    "        (\"1972-08-01T00:00:00Z\", 20000.0),\n",
    "        (\"1972-09-01T00:00:00Z\", 883.0),\n",
    "        (\"1972-10-01T00:00:00Z\", 898.0),\n",
    "        (\"1972-11-01T00:00:00Z\", 957.0),\n",
    "        (\"1972-12-01T00:00:00Z\", 924.0),\n",
    "        (\"1973-01-01T00:00:00Z\", 881.0),\n",
    "        (\"1973-02-01T00:00:00Z\", 837.0),\n",
    "        (\"1973-03-01T00:00:00Z\", 9000.0),\n",
    "    ],\n",
    "    [\"timestamp\", \"value\"],\n",
    ").withColumn(\"group\", lit(\"series1\"))\n",
    "\n",
    "# Run the Anomaly Detector service to look for irregular data\n",
    "anamoly_detector = (\n",
    "    SimpleDetectAnomalies()\n",
    "    .setSubscriptionKey(anomaly_key)\n",
    "    .setLocation(anomaly_loc)\n",
    "    .setTimestampCol(\"timestamp\")\n",
    "    .setValueCol(\"value\")\n",
    "    .setOutputCol(\"anomalies\")\n",
    "    .setGroupbyCol(\"group\")\n",
    "    .setGranularity(\"monthly\")\n",
    ")\n",
    "\n",
    "# Show the full results of the analysis with the anomalies marked as \"True\"\n",
    "display(\n",
    "    anamoly_detector.transform(df).select(\"timestamp\", \"value\", \"anomalies.isAnomaly\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get information from arbitrary web APIs\n",
    "\n",
    "With HTTP on Spark, any web service can be used in your big data pipeline. In this example, we use the [World Bank API](http://api.worldbank.org/v2/country/) to get information about various countries around the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use any requests from the python requests library\n",
    "\n",
    "\n",
    "def world_bank_request(country):\n",
    "    return Request(\n",
    "        \"GET\", \"http://api.worldbank.org/v2/country/{}?format=json\".format(country)\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a dataframe with specifies which countries we want data on\n",
    "df = spark.createDataFrame([(\"br\",), (\"usa\",)], [\"country\"]).withColumn(\n",
    "    \"request\", http_udf(world_bank_request)(col(\"country\"))\n",
    ")\n",
    "\n",
    "# Much faster for big data because of the concurrency :)\n",
    "client = (\n",
    "    HTTPTransformer().setConcurrency(3).setInputCol(\"request\").setOutputCol(\"response\")\n",
    ")\n",
    "\n",
    "# Get the body of the response\n",
    "\n",
    "\n",
    "def get_response_body(resp):\n",
    "    return resp.entity.content.decode()\n",
    "\n",
    "\n",
    "# Show the details of the country data returned\n",
    "display(\n",
    "    client.transform(df).select(\n",
    "        \"country\", udf(get_response_body)(col(\"response\")).alias(\"response\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-synapse-internal"
    ]
   },
   "source": [
    "## Azure AI search sample\n",
    "\n",
    "In this example, we show how you can enrich data using Cognitive Skills and write to an Azure Search Index using SynapseML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-synapse-internal"
    ]
   },
   "outputs": [],
   "source": [
    "search_service = \"mmlspark-azure-search\"\n",
    "search_index = \"test-33467690\"\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\n",
    "            \"upload\",\n",
    "            \"0\",\n",
    "            \"https://mmlspark.blob.core.windows.net/datasets/DSIR/test1.jpg\",\n",
    "        ),\n",
    "        (\n",
    "            \"upload\",\n",
    "            \"1\",\n",
    "            \"https://mmlspark.blob.core.windows.net/datasets/DSIR/test2.jpg\",\n",
    "        ),\n",
    "    ],\n",
    "    [\"searchAction\", \"id\", \"url\"],\n",
    ")\n",
    "\n",
    "tdf = (\n",
    "    AnalyzeImage()\n",
    "    .setSubscriptionKey(service_key)\n",
    "    .setLocation(service_loc)\n",
    "    .setImageUrlCol(\"url\")\n",
    "    .setOutputCol(\"analyzed\")\n",
    "    .setErrorCol(\"errors\")\n",
    "    .setVisualFeatures(\n",
    "        [\"Categories\", \"Tags\", \"Description\", \"Faces\", \"ImageType\", \"Color\", \"Adult\"]\n",
    "    )\n",
    "    .transform(df)\n",
    "    .select(\"*\", \"analyzed.*\")\n",
    "    .drop(\"errors\", \"analyzed\")\n",
    ")\n",
    "\n",
    "tdf.writeToAzureSearch(\n",
    "    subscriptionKey=search_key,\n",
    "    actionCol=\"searchAction\",\n",
    "    serviceName=search_service,\n",
    "    indexName=search_index,\n",
    "    keyCol=\"id\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "AI Services - Overview.ipynb",
   "notebookOrigID": 3559341777151035,
   "widgets": {}
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}