{
  "paragraphs": [
    {
      "user": "anonymous",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1544037871495_289495615",
      "id": "paragraph_1544037871495_289495615",
      "dateCreated": "2018-12-05T19:33:54+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:883",
      "text": "%spark.dep\n// include the azure mmlspark dependency\nz.reset()\nz.load(\"Azure:mmlspark:0.15\")",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@160aba7e\n"
          }
        ]
      },
      "runtimeInfos": {}
    },
    {
      "text": "%spark.pyspark\r\n\r\n# Zeppelin needs the path to be update manually to find mmlspark library\r\nimport sys\r\nsys.path.extend(sc.getConf().get(\"spark.jars\").split(\",\"))\r\n\r\n\r\nimport mmlspark\r\nfrom pyspark.sql.types import *\r\nfrom pyspark.sql import SparkSession\r\n\r\nfrom pyspark.sql.functions import length, col\r\n\r\nspark = SparkSession.builder.appName(\"SimpleContServing\").getOrCreate()\r\nsc = spark.sparkContext\r\nsc.setLogLevel(\"WARN\")\r\n\r\nprint(\"creating df\")\r\ndf = spark.readStream.continuousServer() \\\r\n    .address(\"0.0.0.0\", 8888, \"my_api\") \\\r\n    .load() \\\r\n    .parseRequest(StructType().add(\"foo\", StringType()).add(\"bar\", IntegerType()))\r\n\r\nreplies = df.withColumn(\"fooLength\", length(col(\"foo\")))\\\r\n    .makeReply(\"fooLength\")\r\n\r\nprint(\"creating server\")\r\nserver = replies\\\r\n    .writeStream \\\r\n    .continuousServer() \\\r\n    .trigger(continuous=\"1 second\") \\\r\n    .replyTo(\"my_api\") \\\r\n    .queryName(\"my_query\") \\\r\n    .option(\"checkpointLocation\", \"file:///tmp/checkpoints\")\r\n\r\nprint(\"starting server\")\r\nquery = server.start()\r\nprint(\"server running\")\r\nquery.awaitTermination()\r\n\r\n# Test \r\n# curl -X POST -d '{\"foo\":\"foolen\", \"bar\":43}' -H \"ContentType: application/json\" http://[[ip address of load balancer]]:8888/",
      "user": "anonymous",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1543566866130_-1379379014",
      "id": "paragraph_1543566866130_-1379379014",
      "dateCreated": "2018-12-05T19:33:58+0000",
      "status": "ABORT",
      "focus": true,
      "$$hashKey": "object:806",
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "creating df\ncreating server\nstarting server\n"
          },
          {
            "type": "TEXT",
            "data": "Fail to execute line 34: query.awaitTermination()\r\nTraceback (most recent call last):\n  File \"/tmp/1544038219478-0/zeppelin_python.py\", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 34, in <module>\n  File \"/opt/spark/python/pyspark/sql/streaming.py\", line 103, in awaitTermination\n    return self._jsq.awaitTermination()\n  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n    answer = self.gateway_client.send_command(command)\n  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n    response = connection.send_command(command)\n  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n  File \"/usr/lib/python2.7/socket.py\", line 447, in readline\n    data = self._sock.recv(self._rbufsize)\n  File \"/opt/spark/python/pyspark/context.py\", line 252, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"
          }
        ]
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "http://56e17c2acc2e:4040/jobs/job?id=0"
          ],
          "interpreterSettingId": "spark"
        }
      }
    },
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1544038163179_625795270",
      "id": "paragraph_1544038163179_625795270",
      "dateCreated": "2018-12-05T19:29:23+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:1143"
    }
  ],
  "name": "sparkserving",
  "id": "2DZFNGU8A",
  "defaultInterpreterGroup": "spark",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}
